{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aef073b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from flask import Flask, request, jsonify\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "from transformers import pipeline\n",
    "import glob\n",
    "from collections import Counter\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8285da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\GIGABYTE\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\GIGABYTE\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Tải các tài nguyên NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Khởi tạo Flask\n",
    "app = Flask(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e36d23bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker\n",
    "# Thiết lập logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Khởi tạo spell checker\n",
    "spell = SpellChecker()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8179722e",
   "metadata": {},
   "source": [
    "# Tiền xử lý"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e875d470",
   "metadata": {},
   "source": [
    "# PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1213f709",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd0b0478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Đã đọc xong toàn bộ file PDF và ghi vào file TXT.\n"
     ]
    }
   ],
   "source": [
    "# Đường dẫn tới thư mục chứa các file PDF\n",
    "pdf_folder = r'D:\\NLP\\medicalbot\\dataset\\pdf'\n",
    "output_txt = r'D:\\NLP\\medicalbot\\dataset\\pdf_data.txt'\n",
    "\n",
    "with open(output_txt, 'w', encoding='utf-8') as out_file:\n",
    "    for filename in os.listdir(pdf_folder):\n",
    "        if filename.lower().endswith('.pdf'):\n",
    "            pdf_path = os.path.join(pdf_folder, filename)\n",
    "            doc = fitz.open(pdf_path)\n",
    "            \n",
    "            out_file.write(f\"\\n--- Nội dung từ file: {filename} ---\\n\\n\")\n",
    "            \n",
    "            for page in doc:\n",
    "                text = page.get_text()\n",
    "                out_file.write(text + '\\n')\n",
    "            \n",
    "            doc.close()\n",
    "\n",
    "print(\"✅ Đã đọc xong toàn bộ file PDF và ghi vào file TXT.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be0696ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Hàm làm sạch văn bản (giả định, bạn có thể tùy chỉnh)\n",
    "def clean_text(text):\n",
    "    import re\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Thay nhiều khoảng trắng bằng một\n",
    "    text = re.sub(r'[^\\w\\s.,!?]', '', text)  # Xóa ký tự đặc biệt\n",
    "    return text.strip()\n",
    "\n",
    "# Hàm chia đoạn văn bản từ tệp TXT sử dụng RecursiveCharacterTextSplitter\n",
    "def clean_and_split_txt(txt_path, chunk_size=1000, chunk_overlap=200):\n",
    "    try:\n",
    "        # Đọc nội dung tệp TXT\n",
    "        with open(txt_path, 'r', encoding='utf-8') as f:\n",
    "            raw_text = f.read()\n",
    "\n",
    "        # Làm sạch văn bản\n",
    "        cleaned_text = clean_text(raw_text)\n",
    "\n",
    "        # Khởi tạo RecursiveCharacterTextSplitter\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            length_function=len,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],  # Thứ tự ưu tiên chia đoạn\n",
    "        )\n",
    "\n",
    "        # Chia văn bản thành các đoạn\n",
    "        chunks = text_splitter.split_text(cleaned_text)\n",
    "\n",
    "        return chunks\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing {txt_path}: {str(e)}\")\n",
    "        return []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2cf7eb52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-08 17:32:55,564 - INFO - Extracted 57737 chunks\n"
     ]
    }
   ],
   "source": [
    "# Ví dụ sử dụng\n",
    "if __name__ == \"__main__\":\n",
    "    txt_path = r\"D:\\NLP\\medicalbot\\dataset\\pdf_data.txt\"\n",
    "    chunks = clean_and_split_txt(txt_path)\n",
    "    logging.info(f\"Extracted {len(chunks)} chunks\")\n",
    "    # (Tùy chọn) Lưu các đoạn vào tệp hoặc xử lý tiếp\n",
    "    with open(txt_path.replace(\".txt\", \"_chunks.txt\"), 'w', encoding='utf-8') as f:\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            f.write(f\"Chunk {i+1}:\\n{chunk}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953ef0b2",
   "metadata": {},
   "source": [
    "# Q&A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e28d2c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = re.sub(r'\\s+', ' ', text.strip())  # Loại bỏ khoảng trắng thừa\n",
    "    text = re.sub(r'[^\\w\\s.,!?]', '', text)  # Loại bỏ ký tự đặc biệt\n",
    "    text = text.lower()  # Chuẩn hóa chữ thường\n",
    "    # Sửa lỗi chính tả\n",
    "    words = text.split()\n",
    "    corrected_words = [spell.correction(word) if spell.correction(word) else word for word in words]\n",
    "    text = ' '.join(corrected_words)\n",
    "    # Sửa lỗi cụ thể từ bộ dữ liệu\n",
    "    corrections = {\n",
    "        'ejackulated': 'ejaculated',\n",
    "        'turk': 'turkey',\n",
    "        'cronic': 'chronic',\n",
    "        'bhp': 'benign prostatic hyperplasia'\n",
    "    }\n",
    "    for wrong, correct in corrections.items():\n",
    "        text = text.replace(wrong, correct)\n",
    "    return text\n",
    "\n",
    "# Hàm kiểm tra chất lượng Q&A\n",
    "def check_qa_quality(df):\n",
    "    report = {}\n",
    "    \n",
    "    # Kiểm tra nhãn\n",
    "    labels = df['label'].value_counts()\n",
    "    report['label_distribution'] = labels.to_dict()\n",
    "    \n",
    "    # Kiểm tra độ dài câu hỏi và trả lời\n",
    "    df['question_length'] = df['short_question'].apply(lambda x: len(x.split()) if isinstance(x, str) else 0)\n",
    "    df['answer_length'] = df['short_answer'].apply(lambda x: len(x.split()) if isinstance(x, str) else 0)\n",
    "    report['question_length_stats'] = {\n",
    "        'mean': df['question_length'].mean(),\n",
    "        'min': df['question_length'].min(),\n",
    "        'max': df['question_length'].max()\n",
    "    }\n",
    "    report['answer_length_stats'] = {\n",
    "        'mean': df['answer_length'].mean(),\n",
    "        'min': df['answer_length'].min(),\n",
    "        'max': df['answer_length'].max()\n",
    "    }\n",
    "    \n",
    "    # Kiểm tra giá trị thiếu\n",
    "    report['missing_values'] = df.isnull().sum().to_dict()\n",
    "    \n",
    "    # Phân tích tags\n",
    "    all_tags = []\n",
    "    for tags in df['tags'].dropna():\n",
    "        tags_list = eval(tags) if tags.startswith('[') else tags.split(',')\n",
    "        all_tags.extend([tag.strip().lower() for tag in tags_list])\n",
    "    report['top_tags'] = Counter(all_tags).most_common(10)\n",
    "    \n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "998f0d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_qa_dataset(input_csv, output_csv):\n",
    "    logging.info(\"Processing Q&A dataset...\")\n",
    "    \n",
    "    # Đọc dữ liệu\n",
    "    try:\n",
    "        df = pd.read_csv(input_csv)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error reading {input_csv}: {str(e)}\")\n",
    "        return None, None\n",
    "    \n",
    "    # Kiểm tra nhãn\n",
    "    if not all(df['label'] == 1.0):\n",
    "        logging.warning(\"Found labels other than 1.0, filtering to keep only label 1.0\")\n",
    "        df = df[df['label'] == 1.0]\n",
    "    \n",
    "    # Làm sạch dữ liệu\n",
    "    df['short_question'] = df['short_question'].apply(clean_text)\n",
    "    df['short_answer'] = df['short_answer'].apply(clean_text)\n",
    "    df['tags'] = df['tags'].fillna('[]')  # Thay giá trị thiếu trong tags bằng danh sách rỗng\n",
    "    \n",
    "    # Loại bỏ các mẫu không hợp lệ\n",
    "    df = df[df['short_question'].str.len() > 5]  # Loại câu hỏi quá ngắn\n",
    "    df = df[df['short_answer'].str.len() > 10]   # Loại câu trả lời quá ngắn\n",
    "    \n",
    "    # Chuyển đổi định dạng cho tinh chỉnh\n",
    "    df['text'] = df.apply(lambda row: f\"Question: {row['short_question']}\\nTags: {row['tags']}\\nAnswer: {row['short_answer']}\", axis=1)\n",
    "    \n",
    "    # Kiểm tra chất lượng\n",
    "    quality_report = check_qa_quality(df)\n",
    "    \n",
    "    # Lưu dữ liệu đã xử lý\n",
    "    df[['text', 'short_question', 'short_answer', 'tags', 'label']].to_csv(output_csv, index=False)\n",
    "    logging.info(f\"Processed Q&A dataset saved to {output_csv}\")\n",
    "    \n",
    "    return df, quality_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a963096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Đường dẫn dữ liệu\n",
    "qa_input = r\"D:\\NLP\\medicalbot\\dataset\\qa_base\\validation_data_chatbot.csv\"\n",
    "qa_output = r\"D:\\NLP\\medicalbot\\dataset\\qa_clear\\clear_val_data1111.csv\"\n",
    "\n",
    "# Xử lý Q&A\n",
    "qa_df, qa_report = process_qa_dataset(qa_input, qa_output)\n",
    "if qa_df is not None:\n",
    "    print(\"Q&A Processing Report:\")\n",
    "    print(f\"Number of samples: {len(qa_df)}\")\n",
    "    print(f\"Label distribution: {qa_report['label_distribution']}\")\n",
    "    print(f\"Question length stats: {qa_report['question_length_stats']}\")\n",
    "    print(f\"Answer length stats: {qa_report['answer_length_stats']}\")\n",
    "    print(f\"Missing values: {qa_report['missing_values']}\")\n",
    "    print(f\"Top 10 tags: {qa_report['top_tags']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2b84a469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã xử lý cột 'text' và lưu vào file 'clear_val_data_cleaned.csv'\n"
     ]
    }
   ],
   "source": [
    "# Đọc file CSV\n",
    "df = pd.read_csv(r'D:\\NLP\\medicalbot\\dataset\\qa_clear\\clear_val_data.csv')\n",
    "\n",
    "def clean_text(text):\n",
    "    # Tìm nội dung trong cặp dấu ngoặc\n",
    "    match = re.search(r'\\[(.*?)\\]', text)\n",
    "    if match:\n",
    "        # Lấy nội dung trong dấu ngoặc\n",
    "        content = match.group(1)\n",
    "        # Xóa dấu nháy và chia thành danh sách, sau đó nối lại bằng \", \"\n",
    "        cleaned_content = ', '.join(item.strip(\"' \") for item in content.split(\"' '\"))\n",
    "        # Thay thế cặp dấu ngoặc và nội dung bằng nội dung đã xử lý\n",
    "        cleaned_text = re.sub(r'\\[.*?\\]', cleaned_content, text)\n",
    "    else:\n",
    "        # Nếu không có cặp dấu ngoặc, giữ nguyên text\n",
    "        cleaned_text = text\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "# Áp dụng hàm clean_text vào cột 'text'\n",
    "df['text'] = df['text'].apply(clean_text)\n",
    "\n",
    "# Lưu lại file CSV đã chỉnh sửa\n",
    "df.to_csv(r'D:\\NLP\\medicalbot\\dataset\\qa_clear\\clear_val_data.csv', index=False)\n",
    "\n",
    "print(\"Đã xử lý cột 'text' và lưu vào file 'clear_val_data_cleaned.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef62b073",
   "metadata": {},
   "source": [
    "# Phần Chatbot Q&A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "113054ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-11 09:44:38,712 - INFO - PyTorch version 2.5.1+cu121 available.\n",
      "2025-05-11 09:44:38,712 - INFO - TensorFlow version 2.19.0 available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tập huấn luyện: 23663 mẫu\n",
      "Tập kiểm tra: 5915 mẫu\n",
      "Ví dụ prompt đầu tiên:\n",
      "Question: can an antibiotic through an i give you a rash a couple days later\n",
      "Tags: rash, antibiotic\n",
      "Answer: yes it can even after you have finished the prescription for antibiotics\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# Đọc file dữ liệu\n",
    "train_df = pd.read_csv(r\"D:\\NLP\\medicalbot\\dataset\\qa_clear\\clear_train_data.csv\")\n",
    "val_df = pd.read_csv(r\"D:\\NLP\\medicalbot\\dataset\\qa_clear\\clear_val_data.csv\")\n",
    "\n",
    "# Chuyển thành định dạng Hugging Face Dataset\n",
    "train_dataset = Dataset.from_pandas(train_df[['text']])\n",
    "val_dataset = Dataset.from_pandas(val_df[['text']])\n",
    "\n",
    "# In thông tin\n",
    "print(f\"Tập huấn luyện: {len(train_dataset)} mẫu\")\n",
    "print(f\"Tập kiểm tra: {len(val_dataset)} mẫu\")\n",
    "print(\"Ví dụ prompt đầu tiên:\")\n",
    "print(train_dataset[0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "837d6a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is CUDA available: True\n",
      "GPU name: NVIDIA GeForce RTX 3050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"Is CUDA available:\", torch.cuda.is_available())\n",
    "print(\"GPU name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29179e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Available VRAM: 4.29 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\GIGABYTE\\anaconda3\\envs\\env_medbot\\lib\\site-packages\\accelerate\\utils\\modeling.py:1569: UserWarning: Current model requires 64 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n",
      "2025-05-11 09:45:43,444 - INFO - Based on the current allocation process, no modules could be assigned to the following devices due to insufficient memory:\n",
      "  - 0: 646979584 bytes required\n",
      "These minimum requirements are specific to this allocation attempt and may vary. Consider increasing the available memory for these devices to at least the specified minimum, or adjusting the model config.\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "c:\\Users\\GIGABYTE\\anaconda3\\envs\\env_medbot\\lib\\site-packages\\accelerate\\utils\\modeling.py:1569: UserWarning: Current model requires 128 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n",
      "2025-05-11 09:45:49,638 - INFO - Based on the current allocation process, no modules could be assigned to the following devices due to insufficient memory:\n",
      "  - 0: 647192576 bytes required\n",
      "These minimum requirements are specific to this allocation attempt and may vary. Consider increasing the available memory for these devices to at least the specified minimum, or adjusting the model config.\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 502.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 10.53 GiB is allocated by PyTorch, and 42.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 101\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# Tiếp tục huấn luyện từ checkpoint\u001b[39;00m\n\u001b[0;32m    100\u001b[0m checkpoint_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:/NLP/medicalbot/lora_finetuned_llama/checkpoint-5916\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 101\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;66;03m# Lưu mô hình và tokenizer\u001b[39;00m\n\u001b[0;32m    104\u001b[0m model\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./lora_finetuned_llama\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\GIGABYTE\\anaconda3\\envs\\env_medbot\\lib\\site-packages\\transformers\\trainer.py:2245\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2243\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2244\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2246\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2250\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\GIGABYTE\\anaconda3\\envs\\env_medbot\\lib\\site-packages\\transformers\\trainer.py:2560\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2553\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2554\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m   2555\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2556\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[0;32m   2557\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[0;32m   2558\u001b[0m )\n\u001b[0;32m   2559\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m-> 2560\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2562\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2563\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2564\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2565\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2566\u001b[0m ):\n\u001b[0;32m   2567\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2568\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\GIGABYTE\\anaconda3\\envs\\env_medbot\\lib\\site-packages\\transformers\\trainer.py:3736\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[0;32m   3733\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   3735\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 3736\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3738\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[0;32m   3739\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   3740\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   3741\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   3742\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\GIGABYTE\\anaconda3\\envs\\env_medbot\\lib\\site-packages\\transformers\\trainer.py:3801\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[0;32m   3799\u001b[0m         loss_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[0;32m   3800\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs}\n\u001b[1;32m-> 3801\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m   3802\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\GIGABYTE\\anaconda3\\envs\\env_medbot\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\GIGABYTE\\anaconda3\\envs\\env_medbot\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\GIGABYTE\\anaconda3\\envs\\env_medbot\\lib\\site-packages\\accelerate\\utils\\operations.py:814\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    813\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 814\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\GIGABYTE\\anaconda3\\envs\\env_medbot\\lib\\site-packages\\accelerate\\utils\\operations.py:802\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    801\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n",
      "File \u001b[1;32mc:\\Users\\GIGABYTE\\anaconda3\\envs\\env_medbot\\lib\\site-packages\\torch\\amp\\autocast_mode.py:44\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[1;32m---> 44\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\GIGABYTE\\anaconda3\\envs\\env_medbot\\lib\\site-packages\\peft\\peft_model.py:1757\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.forward\u001b[1;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[0;32m   1755\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1756\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[1;32m-> 1757\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model(\n\u001b[0;32m   1758\u001b[0m             input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   1759\u001b[0m             attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   1760\u001b[0m             inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m   1761\u001b[0m             labels\u001b[38;5;241m=\u001b[39mlabels,\n\u001b[0;32m   1762\u001b[0m             output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   1763\u001b[0m             output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   1764\u001b[0m             return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m   1765\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1766\u001b[0m         )\n\u001b[0;32m   1768\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n\u001b[0;32m   1769\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1770\u001b[0m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\GIGABYTE\\anaconda3\\envs\\env_medbot\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\GIGABYTE\\anaconda3\\envs\\env_medbot\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\GIGABYTE\\anaconda3\\envs\\env_medbot\\lib\\site-packages\\peft\\tuners\\tuners_utils.py:193\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[1;32m--> 193\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\GIGABYTE\\anaconda3\\envs\\env_medbot\\lib\\site-packages\\accelerate\\hooks.py:176\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[1;34m(module, *args, **kwargs)\u001b[0m\n\u001b[0;32m    174\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 176\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[1;32mc:\\Users\\GIGABYTE\\anaconda3\\envs\\env_medbot\\lib\\site-packages\\transformers\\utils\\generic.py:965\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    962\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    964\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 965\u001b[0m     output \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    966\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[0;32m    967\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[1;32mc:\\Users\\GIGABYTE\\anaconda3\\envs\\env_medbot\\lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[0;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\GIGABYTE\\anaconda3\\envs\\env_medbot\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:837\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[0;32m    835\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n\u001b[0;32m    836\u001b[0m slice_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mslice\u001b[39m(\u001b[38;5;241m-\u001b[39mlogits_to_keep, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(logits_to_keep, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m logits_to_keep\n\u001b[1;32m--> 837\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mslice_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    839\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    840\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\GIGABYTE\\anaconda3\\envs\\env_medbot\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\GIGABYTE\\anaconda3\\envs\\env_medbot\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\GIGABYTE\\anaconda3\\envs\\env_medbot\\lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 502.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 10.53 GiB is allocated by PyTorch, and 42.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import load_dataset\n",
    "import numpy\n",
    "import os\n",
    "\n",
    "# Giải phóng VRAM\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "torch.cuda.empty_cache()  # Giải phóng VRAM trước khi huấn luyện\n",
    "\n",
    "# Thêm numpy.dtypes.UInt32DType, numpy.dtype, numpy.ndarray và numpy._core.multiarray._reconstruct vào danh sách an toàn\n",
    "torch.serialization.add_safe_globals([\n",
    "    numpy.dtypes.UInt32DType,\n",
    "    numpy.dtype,\n",
    "    numpy.ndarray,\n",
    "    numpy._core.multiarray._reconstruct\n",
    "])\n",
    "\n",
    "# Kiểm tra GPU và VRAM\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Available VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "# Tải mô hình và tokenizer\n",
    "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"  # Thay bằng đường dẫn thực tế\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,  # Sử dụng float16 để tiết kiệm bộ nhớ\n",
    "    device_map=\"auto\",         # Tự động phân bổ lên GPU\n",
    "    trust_remote_code=True\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Đặt pad_token\n",
    "\n",
    "# Tải dataset từ file CSV\n",
    "dataset = load_dataset(\"csv\", data_files={\"train\": r\"D:\\NLP\\medicalbot\\dataset\\qa_clear\\clear_train_data.csv\", \"validation\": r\"D:\\NLP\\medicalbot\\dataset\\qa_clear\\clear_val_data.csv\"})\n",
    "train_dataset = dataset[\"train\"]\n",
    "val_dataset = dataset[\"validation\"]\n",
    "\n",
    "# Hàm tokenize cho dataset\n",
    "def tokenize_function(examples):\n",
    "    texts = examples[\"text\"]\n",
    "    tokenized = tokenizer(\n",
    "        texts,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512,  # Có thể giảm xuống 256 nếu cần tiết kiệm VRAM\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n",
    "    tokenized[\"labels\"][tokenized[\"labels\"] == tokenizer.pad_token_id] = -100\n",
    "    return tokenized\n",
    "\n",
    "# Áp dụng tokenize cho dataset\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=train_dataset.column_names)\n",
    "val_dataset = val_dataset.map(tokenize_function, batched=True, remove_columns=val_dataset.column_names)\n",
    "\n",
    "# Định dạng dataset để huấn luyện\n",
    "train_dataset.set_format(\"torch\")\n",
    "val_dataset.set_format(\"torch\")\n",
    "\n",
    "# Cấu hình LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Cấu hình huấn luyện\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./lora_finetuned_llama\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=4,  # Theo yêu cầu\n",
    "    per_device_eval_batch_size=4,   # Theo yêu cầu\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,                     # Bật mixed precision để tối ưu GPU\n",
    "    save_strategy=\"epoch\",         # Lưu mô hình sau mỗi epoch\n",
    "    load_best_model_at_end=True,\n",
    "    dataloader_pin_memory=True,    # Tăng tốc truyền dữ liệu sang GPU\n",
    "    dataloader_num_workers=4,      # Sử dụng nhiều worker để tải dữ liệu\n",
    "    logging_steps=10,              # Ghi log thường xuyên để theo dõi\n",
    ")\n",
    "\n",
    "# Khởi tạo Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=None  # Không cần collator vì đã padding trong tokenize\n",
    ")\n",
    "\n",
    "# Tiếp tục huấn luyện từ checkpoint\n",
    "checkpoint_path = \"D:/NLP/medicalbot/lora_finetuned_llama/checkpoint-5916\"\n",
    "trainer.train(resume_from_checkpoint=checkpoint_path)\n",
    "\n",
    "# Lưu mô hình và tokenizer\n",
    "model.save_pretrained(\"./lora_finetuned_llama\")\n",
    "tokenizer.save_pretrained(\"./lora_finetuned_llama\")\n",
    "\n",
    "# Giải phóng VRAM sau huấn luyện\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df37003f",
   "metadata": {},
   "source": [
    "# Phần RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90872ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "\n",
    "with pdfplumber.open(r\"D:\\NLP\\medicalbot\\Gale Encyclopedia of Medicine. Vol. 1. 2nd ed.pdf\") as pdf:\n",
    "    text = \"\"\n",
    "    for page in pdf.pages:\n",
    "        text += page.extract_text()\n",
    "    with open(\"output.txt\", \"w\", encoding=\"utf-8\") as txt_file:\n",
    "        txt_file.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d8cded",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "loader = TextLoader('output.txt')\n",
    "documents = loader.load()\n",
    "\n",
    "# Chia nhỏ tài liệu\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# Tạo vector store với Chroma và embedding\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vectorstore = Chroma.from_documents(texts, embeddings)\n",
    "\n",
    "# Khởi tạo mô hình ngôn ngữ (sử dụng pipeline nhỏ để demo)\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"gpt2\",  # Thay bằng mô hình mạnh hơn như Llama nếu có GPU\n",
    "    task=\"text-generation\",\n",
    "    pipeline_kwargs={\"max_new_tokens\": 150}\n",
    ")\n",
    "\n",
    "# Tạo chuỗi RetrievalQA\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 3}),\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "# Hàm lấy câu trả lời từ RAG\n",
    "def get_rag_answer(user_question):\n",
    "    result = qa_chain({\"query\": user_question})\n",
    "    answer = result['result']\n",
    "    return f\"Based on the Gale Encyclopedia of Medicine: {answer}\\n\\nPlease consult a healthcare professional for personalized advice.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3747b1",
   "metadata": {},
   "source": [
    "# Kết hợp cả hai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5088af28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hybrid_answer(user_question):\n",
    "    # Thử tìm câu trả lời từ dữ liệu hội thoại trước\n",
    "    conv_answer, conv_score = get_conversation_answer(user_question)\n",
    "    \n",
    "    if conv_answer and conv_score > 0.5:  # Nếu câu trả lời hội thoại đủ tốt\n",
    "        return f\"Here's a quick answer: {conv_answer}\\n\\nFor more details, consult a healthcare professional.\"\n",
    "    else:\n",
    "        # Sử dụng RAG để lấy thông tin chi tiết\n",
    "        rag_answer = get_rag_answer(user_question)\n",
    "        if conv_answer:\n",
    "            return f\"Here's a quick answer: {conv_answer}\\n\\nFor more details: {rag_answer}\"\n",
    "        return rag_answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1032e0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API endpoint cho chatbot\n",
    "@app.route('/chat', methods=['POST'])\n",
    "def chat():\n",
    "    user_input = request.json.get('message', '')\n",
    "    response = get_hybrid_answer(user_input)\n",
    "    return jsonify({'response': response})\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_medbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

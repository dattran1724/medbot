{
  "best_global_step": 5916,
  "best_metric": 2.7013356685638428,
  "best_model_checkpoint": "./lora_finetuned_llama\\checkpoint-5916",
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 5916,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0016903313049357674,
      "grad_norm": 1.3638856410980225,
      "learning_rate": 0.00019989858012170385,
      "loss": 3.5629,
      "step": 10
    },
    {
      "epoch": 0.0033806626098715348,
      "grad_norm": 1.4806543588638306,
      "learning_rate": 0.00019978589136804148,
      "loss": 3.3822,
      "step": 20
    },
    {
      "epoch": 0.005070993914807302,
      "grad_norm": 40.447845458984375,
      "learning_rate": 0.0001997295469912103,
      "loss": 3.2284,
      "step": 30
    },
    {
      "epoch": 0.0067613252197430695,
      "grad_norm": 1.2949650287628174,
      "learning_rate": 0.0001996168582375479,
      "loss": 3.0906,
      "step": 40
    },
    {
      "epoch": 0.008451656524678837,
      "grad_norm": 1.5119657516479492,
      "learning_rate": 0.00019950416948388554,
      "loss": 2.9876,
      "step": 50
    },
    {
      "epoch": 0.010141987829614604,
      "grad_norm": 1.7082430124282837,
      "learning_rate": 0.00019939148073022314,
      "loss": 2.7704,
      "step": 60
    },
    {
      "epoch": 0.011832319134550372,
      "grad_norm": 1.6210719347000122,
      "learning_rate": 0.00019927879197656075,
      "loss": 2.7937,
      "step": 70
    },
    {
      "epoch": 0.013522650439486139,
      "grad_norm": 1.663744568824768,
      "learning_rate": 0.00019916610322289838,
      "loss": 2.9906,
      "step": 80
    },
    {
      "epoch": 0.015212981744421906,
      "grad_norm": 1.5529592037200928,
      "learning_rate": 0.00019905341446923598,
      "loss": 3.1101,
      "step": 90
    },
    {
      "epoch": 0.016903313049357674,
      "grad_norm": 1.1969051361083984,
      "learning_rate": 0.00019894072571557362,
      "loss": 2.9363,
      "step": 100
    },
    {
      "epoch": 0.01859364435429344,
      "grad_norm": 1.2740638256072998,
      "learning_rate": 0.00019882803696191122,
      "loss": 3.0684,
      "step": 110
    },
    {
      "epoch": 0.02028397565922921,
      "grad_norm": 1.3187031745910645,
      "learning_rate": 0.00019871534820824883,
      "loss": 2.975,
      "step": 120
    },
    {
      "epoch": 0.021974306964164976,
      "grad_norm": 1.4888213872909546,
      "learning_rate": 0.00019860265945458646,
      "loss": 2.8359,
      "step": 130
    },
    {
      "epoch": 0.023664638269100743,
      "grad_norm": 1.6176681518554688,
      "learning_rate": 0.00019848997070092406,
      "loss": 2.9462,
      "step": 140
    },
    {
      "epoch": 0.02535496957403651,
      "grad_norm": 2.2022294998168945,
      "learning_rate": 0.00019837728194726167,
      "loss": 2.8864,
      "step": 150
    },
    {
      "epoch": 0.027045300878972278,
      "grad_norm": 1.9361240863800049,
      "learning_rate": 0.0001982645931935993,
      "loss": 2.9442,
      "step": 160
    },
    {
      "epoch": 0.028735632183908046,
      "grad_norm": 1.4133262634277344,
      "learning_rate": 0.0001981519044399369,
      "loss": 2.897,
      "step": 170
    },
    {
      "epoch": 0.030425963488843813,
      "grad_norm": 1.8395832777023315,
      "learning_rate": 0.00019803921568627454,
      "loss": 2.9334,
      "step": 180
    },
    {
      "epoch": 0.03211629479377958,
      "grad_norm": 2.2495744228363037,
      "learning_rate": 0.00019792652693261214,
      "loss": 2.7151,
      "step": 190
    },
    {
      "epoch": 0.03380662609871535,
      "grad_norm": 1.896842122077942,
      "learning_rate": 0.00019781383817894975,
      "loss": 2.9077,
      "step": 200
    },
    {
      "epoch": 0.035496957403651115,
      "grad_norm": 1.4692028760910034,
      "learning_rate": 0.00019770114942528738,
      "loss": 2.8904,
      "step": 210
    },
    {
      "epoch": 0.03718728870858688,
      "grad_norm": 1.4264296293258667,
      "learning_rate": 0.00019758846067162498,
      "loss": 2.9286,
      "step": 220
    },
    {
      "epoch": 0.03887762001352265,
      "grad_norm": 1.74772047996521,
      "learning_rate": 0.00019747577191796262,
      "loss": 2.9092,
      "step": 230
    },
    {
      "epoch": 0.04056795131845842,
      "grad_norm": 1.742782711982727,
      "learning_rate": 0.00019736308316430022,
      "loss": 2.9828,
      "step": 240
    },
    {
      "epoch": 0.042258282623394185,
      "grad_norm": 2.0368924140930176,
      "learning_rate": 0.00019725039441063783,
      "loss": 2.8766,
      "step": 250
    },
    {
      "epoch": 0.04394861392832995,
      "grad_norm": 1.2250396013259888,
      "learning_rate": 0.00019713770565697546,
      "loss": 2.9685,
      "step": 260
    },
    {
      "epoch": 0.04563894523326572,
      "grad_norm": 1.6130692958831787,
      "learning_rate": 0.00019702501690331306,
      "loss": 2.7393,
      "step": 270
    },
    {
      "epoch": 0.04732927653820149,
      "grad_norm": 1.4338982105255127,
      "learning_rate": 0.00019691232814965067,
      "loss": 3.0008,
      "step": 280
    },
    {
      "epoch": 0.049019607843137254,
      "grad_norm": 1.781772494316101,
      "learning_rate": 0.0001967996393959883,
      "loss": 2.867,
      "step": 290
    },
    {
      "epoch": 0.05070993914807302,
      "grad_norm": 1.1610829830169678,
      "learning_rate": 0.0001966869506423259,
      "loss": 2.9244,
      "step": 300
    },
    {
      "epoch": 0.05240027045300879,
      "grad_norm": 2.30660343170166,
      "learning_rate": 0.0001965742618886635,
      "loss": 2.8648,
      "step": 310
    },
    {
      "epoch": 0.054090601757944556,
      "grad_norm": 1.7540242671966553,
      "learning_rate": 0.00019646157313500114,
      "loss": 2.9118,
      "step": 320
    },
    {
      "epoch": 0.055780933062880324,
      "grad_norm": 2.03035831451416,
      "learning_rate": 0.00019634888438133875,
      "loss": 2.795,
      "step": 330
    },
    {
      "epoch": 0.05747126436781609,
      "grad_norm": 1.4700441360473633,
      "learning_rate": 0.00019623619562767635,
      "loss": 2.8028,
      "step": 340
    },
    {
      "epoch": 0.05916159567275186,
      "grad_norm": 1.9048250913619995,
      "learning_rate": 0.00019612350687401399,
      "loss": 2.8449,
      "step": 350
    },
    {
      "epoch": 0.060851926977687626,
      "grad_norm": 1.411820411682129,
      "learning_rate": 0.0001960108181203516,
      "loss": 2.8225,
      "step": 360
    },
    {
      "epoch": 0.0625422582826234,
      "grad_norm": 1.955130934715271,
      "learning_rate": 0.0001958981293666892,
      "loss": 2.8414,
      "step": 370
    },
    {
      "epoch": 0.06423258958755916,
      "grad_norm": 1.6114267110824585,
      "learning_rate": 0.00019578544061302683,
      "loss": 2.9161,
      "step": 380
    },
    {
      "epoch": 0.06592292089249494,
      "grad_norm": 1.6487548351287842,
      "learning_rate": 0.00019567275185936443,
      "loss": 2.7916,
      "step": 390
    },
    {
      "epoch": 0.0676132521974307,
      "grad_norm": 1.895622968673706,
      "learning_rate": 0.00019556006310570207,
      "loss": 2.8916,
      "step": 400
    },
    {
      "epoch": 0.06930358350236647,
      "grad_norm": 1.7494771480560303,
      "learning_rate": 0.00019544737435203967,
      "loss": 2.7484,
      "step": 410
    },
    {
      "epoch": 0.07099391480730223,
      "grad_norm": 2.2308506965637207,
      "learning_rate": 0.00019533468559837728,
      "loss": 2.6826,
      "step": 420
    },
    {
      "epoch": 0.072684246112238,
      "grad_norm": 1.5451751947402954,
      "learning_rate": 0.0001952219968447149,
      "loss": 2.7377,
      "step": 430
    },
    {
      "epoch": 0.07437457741717377,
      "grad_norm": 1.6459178924560547,
      "learning_rate": 0.0001951093080910525,
      "loss": 2.8233,
      "step": 440
    },
    {
      "epoch": 0.07606490872210954,
      "grad_norm": 1.3961609601974487,
      "learning_rate": 0.00019499661933739012,
      "loss": 2.7555,
      "step": 450
    },
    {
      "epoch": 0.0777552400270453,
      "grad_norm": 2.1994130611419678,
      "learning_rate": 0.00019488393058372775,
      "loss": 2.7201,
      "step": 460
    },
    {
      "epoch": 0.07944557133198107,
      "grad_norm": 1.4129977226257324,
      "learning_rate": 0.00019477124183006535,
      "loss": 2.8199,
      "step": 470
    },
    {
      "epoch": 0.08113590263691683,
      "grad_norm": 2.554225444793701,
      "learning_rate": 0.000194658553076403,
      "loss": 2.884,
      "step": 480
    },
    {
      "epoch": 0.08282623394185261,
      "grad_norm": 1.8223509788513184,
      "learning_rate": 0.0001945458643227406,
      "loss": 2.8751,
      "step": 490
    },
    {
      "epoch": 0.08451656524678837,
      "grad_norm": 1.9040981531143188,
      "learning_rate": 0.0001944331755690782,
      "loss": 2.7503,
      "step": 500
    },
    {
      "epoch": 0.08620689655172414,
      "grad_norm": 1.8598824739456177,
      "learning_rate": 0.00019432048681541583,
      "loss": 2.8318,
      "step": 510
    },
    {
      "epoch": 0.0878972278566599,
      "grad_norm": 1.5944548845291138,
      "learning_rate": 0.00019420779806175343,
      "loss": 2.8423,
      "step": 520
    },
    {
      "epoch": 0.08958755916159568,
      "grad_norm": 1.909987211227417,
      "learning_rate": 0.00019409510930809107,
      "loss": 2.7645,
      "step": 530
    },
    {
      "epoch": 0.09127789046653144,
      "grad_norm": 1.3629627227783203,
      "learning_rate": 0.00019398242055442867,
      "loss": 2.805,
      "step": 540
    },
    {
      "epoch": 0.09296822177146721,
      "grad_norm": 1.4740352630615234,
      "learning_rate": 0.00019386973180076628,
      "loss": 2.8504,
      "step": 550
    },
    {
      "epoch": 0.09465855307640297,
      "grad_norm": 0.8724974393844604,
      "learning_rate": 0.0001937570430471039,
      "loss": 2.8929,
      "step": 560
    },
    {
      "epoch": 0.09634888438133875,
      "grad_norm": 1.9992952346801758,
      "learning_rate": 0.0001936443542934415,
      "loss": 2.7415,
      "step": 570
    },
    {
      "epoch": 0.09803921568627451,
      "grad_norm": 1.3454784154891968,
      "learning_rate": 0.00019353166553977915,
      "loss": 2.7128,
      "step": 580
    },
    {
      "epoch": 0.09972954699121028,
      "grad_norm": 1.745734453201294,
      "learning_rate": 0.00019341897678611675,
      "loss": 2.7712,
      "step": 590
    },
    {
      "epoch": 0.10141987829614604,
      "grad_norm": 1.3086310625076294,
      "learning_rate": 0.00019330628803245436,
      "loss": 2.8118,
      "step": 600
    },
    {
      "epoch": 0.10311020960108182,
      "grad_norm": 1.995927333831787,
      "learning_rate": 0.000193193599278792,
      "loss": 2.8067,
      "step": 610
    },
    {
      "epoch": 0.10480054090601758,
      "grad_norm": 1.4104392528533936,
      "learning_rate": 0.0001930809105251296,
      "loss": 2.8449,
      "step": 620
    },
    {
      "epoch": 0.10649087221095335,
      "grad_norm": 2.2837777137756348,
      "learning_rate": 0.0001929682217714672,
      "loss": 2.812,
      "step": 630
    },
    {
      "epoch": 0.10818120351588911,
      "grad_norm": 1.6009749174118042,
      "learning_rate": 0.00019285553301780483,
      "loss": 2.9106,
      "step": 640
    },
    {
      "epoch": 0.10987153482082489,
      "grad_norm": 1.77328360080719,
      "learning_rate": 0.00019274284426414243,
      "loss": 2.8838,
      "step": 650
    },
    {
      "epoch": 0.11156186612576065,
      "grad_norm": 1.6232775449752808,
      "learning_rate": 0.00019263015551048007,
      "loss": 2.8619,
      "step": 660
    },
    {
      "epoch": 0.11325219743069642,
      "grad_norm": 1.288456678390503,
      "learning_rate": 0.00019251746675681767,
      "loss": 2.8808,
      "step": 670
    },
    {
      "epoch": 0.11494252873563218,
      "grad_norm": 1.146612286567688,
      "learning_rate": 0.00019240477800315528,
      "loss": 2.8186,
      "step": 680
    },
    {
      "epoch": 0.11663286004056796,
      "grad_norm": 1.7173056602478027,
      "learning_rate": 0.0001922920892494929,
      "loss": 2.8428,
      "step": 690
    },
    {
      "epoch": 0.11832319134550372,
      "grad_norm": 2.066054105758667,
      "learning_rate": 0.00019217940049583051,
      "loss": 2.8413,
      "step": 700
    },
    {
      "epoch": 0.12001352265043949,
      "grad_norm": 1.5701267719268799,
      "learning_rate": 0.00019206671174216815,
      "loss": 2.829,
      "step": 710
    },
    {
      "epoch": 0.12170385395537525,
      "grad_norm": 1.6726971864700317,
      "learning_rate": 0.00019195402298850575,
      "loss": 2.5861,
      "step": 720
    },
    {
      "epoch": 0.12339418526031103,
      "grad_norm": 1.6647027730941772,
      "learning_rate": 0.00019184133423484336,
      "loss": 2.787,
      "step": 730
    },
    {
      "epoch": 0.1250845165652468,
      "grad_norm": 1.848528504371643,
      "learning_rate": 0.000191728645481181,
      "loss": 2.8076,
      "step": 740
    },
    {
      "epoch": 0.12677484787018256,
      "grad_norm": 1.713807225227356,
      "learning_rate": 0.0001916159567275186,
      "loss": 2.9003,
      "step": 750
    },
    {
      "epoch": 0.12846517917511832,
      "grad_norm": 3.0559048652648926,
      "learning_rate": 0.00019150326797385623,
      "loss": 2.8277,
      "step": 760
    },
    {
      "epoch": 0.13015551048005408,
      "grad_norm": 1.932951807975769,
      "learning_rate": 0.00019139057922019383,
      "loss": 2.7908,
      "step": 770
    },
    {
      "epoch": 0.13184584178498987,
      "grad_norm": 1.49520742893219,
      "learning_rate": 0.00019127789046653144,
      "loss": 2.5914,
      "step": 780
    },
    {
      "epoch": 0.13353617308992563,
      "grad_norm": 1.5666334629058838,
      "learning_rate": 0.00019116520171286907,
      "loss": 2.9167,
      "step": 790
    },
    {
      "epoch": 0.1352265043948614,
      "grad_norm": 1.7628003358840942,
      "learning_rate": 0.00019105251295920667,
      "loss": 2.8426,
      "step": 800
    },
    {
      "epoch": 0.13691683569979715,
      "grad_norm": 1.9407901763916016,
      "learning_rate": 0.0001909398242055443,
      "loss": 2.9538,
      "step": 810
    },
    {
      "epoch": 0.13860716700473294,
      "grad_norm": 1.5138590335845947,
      "learning_rate": 0.0001908271354518819,
      "loss": 2.8592,
      "step": 820
    },
    {
      "epoch": 0.1402974983096687,
      "grad_norm": 1.6866739988327026,
      "learning_rate": 0.00019071444669821951,
      "loss": 2.7365,
      "step": 830
    },
    {
      "epoch": 0.14198782961460446,
      "grad_norm": 1.9626314640045166,
      "learning_rate": 0.00019060175794455715,
      "loss": 2.7777,
      "step": 840
    },
    {
      "epoch": 0.14367816091954022,
      "grad_norm": 1.55782151222229,
      "learning_rate": 0.00019048906919089475,
      "loss": 2.7259,
      "step": 850
    },
    {
      "epoch": 0.145368492224476,
      "grad_norm": 1.622695803642273,
      "learning_rate": 0.00019037638043723238,
      "loss": 2.7492,
      "step": 860
    },
    {
      "epoch": 0.14705882352941177,
      "grad_norm": 1.495359182357788,
      "learning_rate": 0.00019026369168357,
      "loss": 2.8746,
      "step": 870
    },
    {
      "epoch": 0.14874915483434753,
      "grad_norm": 2.1937732696533203,
      "learning_rate": 0.0001901510029299076,
      "loss": 2.6329,
      "step": 880
    },
    {
      "epoch": 0.1504394861392833,
      "grad_norm": 1.842393398284912,
      "learning_rate": 0.00019003831417624523,
      "loss": 2.8037,
      "step": 890
    },
    {
      "epoch": 0.15212981744421908,
      "grad_norm": 1.5255807638168335,
      "learning_rate": 0.00018992562542258283,
      "loss": 2.7485,
      "step": 900
    },
    {
      "epoch": 0.15382014874915484,
      "grad_norm": 1.4448128938674927,
      "learning_rate": 0.00018981293666892046,
      "loss": 2.868,
      "step": 910
    },
    {
      "epoch": 0.1555104800540906,
      "grad_norm": 1.7939162254333496,
      "learning_rate": 0.00018970024791525807,
      "loss": 2.7639,
      "step": 920
    },
    {
      "epoch": 0.15720081135902636,
      "grad_norm": 1.1563678979873657,
      "learning_rate": 0.00018958755916159567,
      "loss": 2.6057,
      "step": 930
    },
    {
      "epoch": 0.15889114266396215,
      "grad_norm": 1.8843427896499634,
      "learning_rate": 0.0001894748704079333,
      "loss": 2.7622,
      "step": 940
    },
    {
      "epoch": 0.1605814739688979,
      "grad_norm": 2.387786626815796,
      "learning_rate": 0.0001893621816542709,
      "loss": 2.7444,
      "step": 950
    },
    {
      "epoch": 0.16227180527383367,
      "grad_norm": 1.869574785232544,
      "learning_rate": 0.00018924949290060854,
      "loss": 2.8628,
      "step": 960
    },
    {
      "epoch": 0.16396213657876943,
      "grad_norm": 2.2911148071289062,
      "learning_rate": 0.00018913680414694615,
      "loss": 2.7431,
      "step": 970
    },
    {
      "epoch": 0.16565246788370522,
      "grad_norm": 1.2393336296081543,
      "learning_rate": 0.00018902411539328375,
      "loss": 2.678,
      "step": 980
    },
    {
      "epoch": 0.16734279918864098,
      "grad_norm": 2.3894577026367188,
      "learning_rate": 0.00018891142663962139,
      "loss": 2.8028,
      "step": 990
    },
    {
      "epoch": 0.16903313049357674,
      "grad_norm": 1.9152830839157104,
      "learning_rate": 0.000188798737885959,
      "loss": 2.9363,
      "step": 1000
    },
    {
      "epoch": 0.1707234617985125,
      "grad_norm": 1.5181834697723389,
      "learning_rate": 0.00018868604913229662,
      "loss": 2.8266,
      "step": 1010
    },
    {
      "epoch": 0.1724137931034483,
      "grad_norm": 1.5110002756118774,
      "learning_rate": 0.00018857336037863423,
      "loss": 2.7025,
      "step": 1020
    },
    {
      "epoch": 0.17410412440838405,
      "grad_norm": 1.8026721477508545,
      "learning_rate": 0.00018846067162497183,
      "loss": 2.7498,
      "step": 1030
    },
    {
      "epoch": 0.1757944557133198,
      "grad_norm": 1.3216944932937622,
      "learning_rate": 0.00018834798287130946,
      "loss": 2.8145,
      "step": 1040
    },
    {
      "epoch": 0.17748478701825557,
      "grad_norm": 1.5485458374023438,
      "learning_rate": 0.00018823529411764707,
      "loss": 2.9001,
      "step": 1050
    },
    {
      "epoch": 0.17917511832319136,
      "grad_norm": 1.782065987586975,
      "learning_rate": 0.0001881226053639847,
      "loss": 2.6923,
      "step": 1060
    },
    {
      "epoch": 0.18086544962812712,
      "grad_norm": 2.096041679382324,
      "learning_rate": 0.0001880099166103223,
      "loss": 2.6373,
      "step": 1070
    },
    {
      "epoch": 0.18255578093306288,
      "grad_norm": 1.8374055624008179,
      "learning_rate": 0.0001878972278566599,
      "loss": 2.8134,
      "step": 1080
    },
    {
      "epoch": 0.18424611223799864,
      "grad_norm": 1.4257221221923828,
      "learning_rate": 0.00018778453910299754,
      "loss": 2.8418,
      "step": 1090
    },
    {
      "epoch": 0.18593644354293443,
      "grad_norm": 1.5161404609680176,
      "learning_rate": 0.00018767185034933515,
      "loss": 2.7653,
      "step": 1100
    },
    {
      "epoch": 0.1876267748478702,
      "grad_norm": 1.4253517389297485,
      "learning_rate": 0.00018755916159567275,
      "loss": 2.6362,
      "step": 1110
    },
    {
      "epoch": 0.18931710615280595,
      "grad_norm": 2.3170154094696045,
      "learning_rate": 0.00018744647284201039,
      "loss": 2.8012,
      "step": 1120
    },
    {
      "epoch": 0.1910074374577417,
      "grad_norm": 1.4164447784423828,
      "learning_rate": 0.000187333784088348,
      "loss": 2.9433,
      "step": 1130
    },
    {
      "epoch": 0.1926977687626775,
      "grad_norm": 1.49468994140625,
      "learning_rate": 0.00018722109533468562,
      "loss": 2.7279,
      "step": 1140
    },
    {
      "epoch": 0.19438810006761326,
      "grad_norm": 3.064082384109497,
      "learning_rate": 0.00018710840658102323,
      "loss": 2.7507,
      "step": 1150
    },
    {
      "epoch": 0.19607843137254902,
      "grad_norm": 1.068271279335022,
      "learning_rate": 0.00018699571782736083,
      "loss": 2.796,
      "step": 1160
    },
    {
      "epoch": 0.19776876267748478,
      "grad_norm": 1.588598608970642,
      "learning_rate": 0.00018688302907369847,
      "loss": 2.6401,
      "step": 1170
    },
    {
      "epoch": 0.19945909398242057,
      "grad_norm": 1.431386113166809,
      "learning_rate": 0.00018677034032003607,
      "loss": 2.9288,
      "step": 1180
    },
    {
      "epoch": 0.20114942528735633,
      "grad_norm": 1.605581521987915,
      "learning_rate": 0.0001866576515663737,
      "loss": 2.7415,
      "step": 1190
    },
    {
      "epoch": 0.2028397565922921,
      "grad_norm": 1.3352082967758179,
      "learning_rate": 0.0001865449628127113,
      "loss": 2.8733,
      "step": 1200
    },
    {
      "epoch": 0.20453008789722785,
      "grad_norm": 1.297742247581482,
      "learning_rate": 0.0001864322740590489,
      "loss": 2.846,
      "step": 1210
    },
    {
      "epoch": 0.20622041920216364,
      "grad_norm": 2.7206528186798096,
      "learning_rate": 0.00018631958530538654,
      "loss": 2.7121,
      "step": 1220
    },
    {
      "epoch": 0.2079107505070994,
      "grad_norm": 1.8386967182159424,
      "learning_rate": 0.00018620689655172415,
      "loss": 2.635,
      "step": 1230
    },
    {
      "epoch": 0.20960108181203516,
      "grad_norm": 2.1384482383728027,
      "learning_rate": 0.00018609420779806178,
      "loss": 2.8456,
      "step": 1240
    },
    {
      "epoch": 0.21129141311697092,
      "grad_norm": 1.5731399059295654,
      "learning_rate": 0.0001859815190443994,
      "loss": 2.6607,
      "step": 1250
    },
    {
      "epoch": 0.2129817444219067,
      "grad_norm": 1.3870517015457153,
      "learning_rate": 0.000185868830290737,
      "loss": 2.633,
      "step": 1260
    },
    {
      "epoch": 0.21467207572684247,
      "grad_norm": 1.58143150806427,
      "learning_rate": 0.00018575614153707462,
      "loss": 2.7273,
      "step": 1270
    },
    {
      "epoch": 0.21636240703177823,
      "grad_norm": 1.5722309350967407,
      "learning_rate": 0.00018564345278341223,
      "loss": 2.7734,
      "step": 1280
    },
    {
      "epoch": 0.21805273833671399,
      "grad_norm": 1.1304627656936646,
      "learning_rate": 0.00018553076402974986,
      "loss": 2.6931,
      "step": 1290
    },
    {
      "epoch": 0.21974306964164977,
      "grad_norm": 1.5662758350372314,
      "learning_rate": 0.00018541807527608747,
      "loss": 2.7528,
      "step": 1300
    },
    {
      "epoch": 0.22143340094658553,
      "grad_norm": 2.4293746948242188,
      "learning_rate": 0.00018530538652242507,
      "loss": 2.7886,
      "step": 1310
    },
    {
      "epoch": 0.2231237322515213,
      "grad_norm": 1.735298991203308,
      "learning_rate": 0.0001851926977687627,
      "loss": 2.7853,
      "step": 1320
    },
    {
      "epoch": 0.22481406355645706,
      "grad_norm": 1.4222004413604736,
      "learning_rate": 0.0001850800090151003,
      "loss": 2.6243,
      "step": 1330
    },
    {
      "epoch": 0.22650439486139284,
      "grad_norm": 1.3250770568847656,
      "learning_rate": 0.0001849673202614379,
      "loss": 2.7374,
      "step": 1340
    },
    {
      "epoch": 0.2281947261663286,
      "grad_norm": 1.3139129877090454,
      "learning_rate": 0.00018485463150777555,
      "loss": 2.8921,
      "step": 1350
    },
    {
      "epoch": 0.22988505747126436,
      "grad_norm": 1.907949447631836,
      "learning_rate": 0.00018474194275411315,
      "loss": 2.7478,
      "step": 1360
    },
    {
      "epoch": 0.23157538877620013,
      "grad_norm": 1.8924243450164795,
      "learning_rate": 0.00018462925400045076,
      "loss": 2.6515,
      "step": 1370
    },
    {
      "epoch": 0.2332657200811359,
      "grad_norm": 1.6558047533035278,
      "learning_rate": 0.0001845165652467884,
      "loss": 2.7689,
      "step": 1380
    },
    {
      "epoch": 0.23495605138607167,
      "grad_norm": 1.992598056793213,
      "learning_rate": 0.000184403876493126,
      "loss": 2.8635,
      "step": 1390
    },
    {
      "epoch": 0.23664638269100743,
      "grad_norm": 1.2562875747680664,
      "learning_rate": 0.0001842911877394636,
      "loss": 2.7903,
      "step": 1400
    },
    {
      "epoch": 0.2383367139959432,
      "grad_norm": 1.8747442960739136,
      "learning_rate": 0.00018417849898580123,
      "loss": 2.8066,
      "step": 1410
    },
    {
      "epoch": 0.24002704530087898,
      "grad_norm": 2.0379185676574707,
      "learning_rate": 0.00018406581023213883,
      "loss": 2.557,
      "step": 1420
    },
    {
      "epoch": 0.24171737660581474,
      "grad_norm": 1.3330157995224,
      "learning_rate": 0.00018395312147847644,
      "loss": 2.6727,
      "step": 1430
    },
    {
      "epoch": 0.2434077079107505,
      "grad_norm": 1.4541347026824951,
      "learning_rate": 0.00018384043272481407,
      "loss": 2.8388,
      "step": 1440
    },
    {
      "epoch": 0.24509803921568626,
      "grad_norm": 1.4131056070327759,
      "learning_rate": 0.00018372774397115168,
      "loss": 2.7619,
      "step": 1450
    },
    {
      "epoch": 0.24678837052062205,
      "grad_norm": 1.8027055263519287,
      "learning_rate": 0.0001836150552174893,
      "loss": 2.7575,
      "step": 1460
    },
    {
      "epoch": 0.2484787018255578,
      "grad_norm": 1.7843904495239258,
      "learning_rate": 0.00018350236646382691,
      "loss": 2.7194,
      "step": 1470
    },
    {
      "epoch": 0.2501690331304936,
      "grad_norm": 1.8308502435684204,
      "learning_rate": 0.00018338967771016452,
      "loss": 2.7175,
      "step": 1480
    },
    {
      "epoch": 0.25185936443542933,
      "grad_norm": 1.9242937564849854,
      "learning_rate": 0.00018327698895650215,
      "loss": 2.7124,
      "step": 1490
    },
    {
      "epoch": 0.2535496957403651,
      "grad_norm": 1.1711878776550293,
      "learning_rate": 0.00018316430020283976,
      "loss": 2.8686,
      "step": 1500
    },
    {
      "epoch": 0.25524002704530085,
      "grad_norm": 1.295661449432373,
      "learning_rate": 0.00018305161144917736,
      "loss": 2.6573,
      "step": 1510
    },
    {
      "epoch": 0.25693035835023664,
      "grad_norm": 2.1838862895965576,
      "learning_rate": 0.000182938922695515,
      "loss": 2.7283,
      "step": 1520
    },
    {
      "epoch": 0.25862068965517243,
      "grad_norm": 1.2762830257415771,
      "learning_rate": 0.0001828262339418526,
      "loss": 2.758,
      "step": 1530
    },
    {
      "epoch": 0.26031102096010816,
      "grad_norm": 1.1588201522827148,
      "learning_rate": 0.00018271354518819023,
      "loss": 2.7311,
      "step": 1540
    },
    {
      "epoch": 0.26200135226504395,
      "grad_norm": 0.9968389868736267,
      "learning_rate": 0.00018260085643452784,
      "loss": 2.7319,
      "step": 1550
    },
    {
      "epoch": 0.26369168356997974,
      "grad_norm": 1.6697410345077515,
      "learning_rate": 0.00018248816768086544,
      "loss": 2.8883,
      "step": 1560
    },
    {
      "epoch": 0.2653820148749155,
      "grad_norm": 1.3968623876571655,
      "learning_rate": 0.00018237547892720307,
      "loss": 2.5855,
      "step": 1570
    },
    {
      "epoch": 0.26707234617985126,
      "grad_norm": 1.8581594228744507,
      "learning_rate": 0.00018226279017354068,
      "loss": 2.6364,
      "step": 1580
    },
    {
      "epoch": 0.268762677484787,
      "grad_norm": 1.53068208694458,
      "learning_rate": 0.00018215010141987828,
      "loss": 2.7918,
      "step": 1590
    },
    {
      "epoch": 0.2704530087897228,
      "grad_norm": 1.8489130735397339,
      "learning_rate": 0.00018203741266621592,
      "loss": 2.8862,
      "step": 1600
    },
    {
      "epoch": 0.27214334009465857,
      "grad_norm": 1.8712838888168335,
      "learning_rate": 0.00018192472391255352,
      "loss": 2.7271,
      "step": 1610
    },
    {
      "epoch": 0.2738336713995943,
      "grad_norm": 1.2185648679733276,
      "learning_rate": 0.00018181203515889115,
      "loss": 2.6376,
      "step": 1620
    },
    {
      "epoch": 0.2755240027045301,
      "grad_norm": 1.4615870714187622,
      "learning_rate": 0.00018169934640522876,
      "loss": 2.8163,
      "step": 1630
    },
    {
      "epoch": 0.2772143340094659,
      "grad_norm": 1.8161582946777344,
      "learning_rate": 0.00018158665765156636,
      "loss": 2.7919,
      "step": 1640
    },
    {
      "epoch": 0.2789046653144016,
      "grad_norm": 1.235028862953186,
      "learning_rate": 0.000181473968897904,
      "loss": 2.5955,
      "step": 1650
    },
    {
      "epoch": 0.2805949966193374,
      "grad_norm": 1.3761135339736938,
      "learning_rate": 0.0001813612801442416,
      "loss": 2.7461,
      "step": 1660
    },
    {
      "epoch": 0.28228532792427313,
      "grad_norm": 1.6518497467041016,
      "learning_rate": 0.00018124859139057923,
      "loss": 2.7009,
      "step": 1670
    },
    {
      "epoch": 0.2839756592292089,
      "grad_norm": 1.4693151712417603,
      "learning_rate": 0.00018113590263691684,
      "loss": 2.8374,
      "step": 1680
    },
    {
      "epoch": 0.2856659905341447,
      "grad_norm": 1.913132667541504,
      "learning_rate": 0.00018102321388325444,
      "loss": 2.7639,
      "step": 1690
    },
    {
      "epoch": 0.28735632183908044,
      "grad_norm": 1.7144498825073242,
      "learning_rate": 0.00018091052512959207,
      "loss": 2.7153,
      "step": 1700
    },
    {
      "epoch": 0.28904665314401623,
      "grad_norm": 1.0702223777770996,
      "learning_rate": 0.00018079783637592968,
      "loss": 2.6532,
      "step": 1710
    },
    {
      "epoch": 0.290736984448952,
      "grad_norm": 1.8263808488845825,
      "learning_rate": 0.0001806851476222673,
      "loss": 2.78,
      "step": 1720
    },
    {
      "epoch": 0.29242731575388775,
      "grad_norm": 1.6065118312835693,
      "learning_rate": 0.00018057245886860492,
      "loss": 2.703,
      "step": 1730
    },
    {
      "epoch": 0.29411764705882354,
      "grad_norm": 1.8854507207870483,
      "learning_rate": 0.00018045977011494252,
      "loss": 2.7662,
      "step": 1740
    },
    {
      "epoch": 0.29580797836375927,
      "grad_norm": 1.5631722211837769,
      "learning_rate": 0.00018034708136128015,
      "loss": 2.6946,
      "step": 1750
    },
    {
      "epoch": 0.29749830966869506,
      "grad_norm": 1.2871971130371094,
      "learning_rate": 0.00018023439260761776,
      "loss": 2.8675,
      "step": 1760
    },
    {
      "epoch": 0.29918864097363085,
      "grad_norm": 1.3477743864059448,
      "learning_rate": 0.0001801217038539554,
      "loss": 2.7739,
      "step": 1770
    },
    {
      "epoch": 0.3008789722785666,
      "grad_norm": 1.8368595838546753,
      "learning_rate": 0.000180009015100293,
      "loss": 2.7245,
      "step": 1780
    },
    {
      "epoch": 0.30256930358350237,
      "grad_norm": 1.4387590885162354,
      "learning_rate": 0.0001798963263466306,
      "loss": 2.7947,
      "step": 1790
    },
    {
      "epoch": 0.30425963488843816,
      "grad_norm": 1.8675715923309326,
      "learning_rate": 0.00017978363759296823,
      "loss": 2.7153,
      "step": 1800
    },
    {
      "epoch": 0.3059499661933739,
      "grad_norm": 1.8800368309020996,
      "learning_rate": 0.00017967094883930584,
      "loss": 2.6371,
      "step": 1810
    },
    {
      "epoch": 0.3076402974983097,
      "grad_norm": 2.3406007289886475,
      "learning_rate": 0.00017955826008564347,
      "loss": 2.7502,
      "step": 1820
    },
    {
      "epoch": 0.3093306288032454,
      "grad_norm": 1.4716567993164062,
      "learning_rate": 0.00017944557133198107,
      "loss": 2.8311,
      "step": 1830
    },
    {
      "epoch": 0.3110209601081812,
      "grad_norm": 1.4540103673934937,
      "learning_rate": 0.00017933288257831868,
      "loss": 2.7237,
      "step": 1840
    },
    {
      "epoch": 0.312711291413117,
      "grad_norm": 1.5314843654632568,
      "learning_rate": 0.0001792201938246563,
      "loss": 2.7639,
      "step": 1850
    },
    {
      "epoch": 0.3144016227180527,
      "grad_norm": 1.3484973907470703,
      "learning_rate": 0.00017910750507099392,
      "loss": 2.7858,
      "step": 1860
    },
    {
      "epoch": 0.3160919540229885,
      "grad_norm": 2.0459659099578857,
      "learning_rate": 0.00017899481631733155,
      "loss": 2.8397,
      "step": 1870
    },
    {
      "epoch": 0.3177822853279243,
      "grad_norm": 1.386204719543457,
      "learning_rate": 0.00017888212756366915,
      "loss": 2.8127,
      "step": 1880
    },
    {
      "epoch": 0.31947261663286003,
      "grad_norm": 1.3829082250595093,
      "learning_rate": 0.00017876943881000676,
      "loss": 2.8461,
      "step": 1890
    },
    {
      "epoch": 0.3211629479377958,
      "grad_norm": 1.1473183631896973,
      "learning_rate": 0.0001786567500563444,
      "loss": 2.7673,
      "step": 1900
    },
    {
      "epoch": 0.32285327924273155,
      "grad_norm": 1.3320882320404053,
      "learning_rate": 0.000178544061302682,
      "loss": 2.6163,
      "step": 1910
    },
    {
      "epoch": 0.32454361054766734,
      "grad_norm": 1.5697262287139893,
      "learning_rate": 0.00017843137254901963,
      "loss": 2.6418,
      "step": 1920
    },
    {
      "epoch": 0.3262339418526031,
      "grad_norm": 1.9401358366012573,
      "learning_rate": 0.00017831868379535723,
      "loss": 2.7363,
      "step": 1930
    },
    {
      "epoch": 0.32792427315753886,
      "grad_norm": 1.620081901550293,
      "learning_rate": 0.00017820599504169484,
      "loss": 2.7336,
      "step": 1940
    },
    {
      "epoch": 0.32961460446247465,
      "grad_norm": 1.865989327430725,
      "learning_rate": 0.00017809330628803247,
      "loss": 2.7192,
      "step": 1950
    },
    {
      "epoch": 0.33130493576741044,
      "grad_norm": 1.4687918424606323,
      "learning_rate": 0.00017798061753437008,
      "loss": 2.7689,
      "step": 1960
    },
    {
      "epoch": 0.33299526707234617,
      "grad_norm": 1.5401480197906494,
      "learning_rate": 0.0001778679287807077,
      "loss": 2.8234,
      "step": 1970
    },
    {
      "epoch": 0.33468559837728196,
      "grad_norm": 1.133057713508606,
      "learning_rate": 0.0001777552400270453,
      "loss": 2.8016,
      "step": 1980
    },
    {
      "epoch": 0.3363759296822177,
      "grad_norm": 1.06587815284729,
      "learning_rate": 0.00017764255127338292,
      "loss": 2.7121,
      "step": 1990
    },
    {
      "epoch": 0.3380662609871535,
      "grad_norm": 1.9866074323654175,
      "learning_rate": 0.00017752986251972055,
      "loss": 2.5906,
      "step": 2000
    },
    {
      "epoch": 0.33975659229208927,
      "grad_norm": 2.24110746383667,
      "learning_rate": 0.00017741717376605815,
      "loss": 2.7604,
      "step": 2010
    },
    {
      "epoch": 0.341446923597025,
      "grad_norm": 1.7973127365112305,
      "learning_rate": 0.0001773044850123958,
      "loss": 2.8911,
      "step": 2020
    },
    {
      "epoch": 0.3431372549019608,
      "grad_norm": 1.5617784261703491,
      "learning_rate": 0.0001771917962587334,
      "loss": 2.7511,
      "step": 2030
    },
    {
      "epoch": 0.3448275862068966,
      "grad_norm": 1.755045771598816,
      "learning_rate": 0.000177079107505071,
      "loss": 2.6789,
      "step": 2040
    },
    {
      "epoch": 0.3465179175118323,
      "grad_norm": 3.6048550605773926,
      "learning_rate": 0.00017696641875140863,
      "loss": 2.7109,
      "step": 2050
    },
    {
      "epoch": 0.3482082488167681,
      "grad_norm": 1.5149099826812744,
      "learning_rate": 0.00017685372999774623,
      "loss": 2.667,
      "step": 2060
    },
    {
      "epoch": 0.34989858012170383,
      "grad_norm": 1.347387671470642,
      "learning_rate": 0.00017674104124408384,
      "loss": 2.7173,
      "step": 2070
    },
    {
      "epoch": 0.3515889114266396,
      "grad_norm": 1.723526120185852,
      "learning_rate": 0.00017662835249042147,
      "loss": 2.6628,
      "step": 2080
    },
    {
      "epoch": 0.3532792427315754,
      "grad_norm": 1.191996693611145,
      "learning_rate": 0.00017651566373675908,
      "loss": 2.6584,
      "step": 2090
    },
    {
      "epoch": 0.35496957403651114,
      "grad_norm": 2.0199034214019775,
      "learning_rate": 0.0001764029749830967,
      "loss": 2.9495,
      "step": 2100
    },
    {
      "epoch": 0.3566599053414469,
      "grad_norm": 1.5609642267227173,
      "learning_rate": 0.00017629028622943431,
      "loss": 2.6842,
      "step": 2110
    },
    {
      "epoch": 0.3583502366463827,
      "grad_norm": 1.7410989999771118,
      "learning_rate": 0.00017617759747577192,
      "loss": 2.7232,
      "step": 2120
    },
    {
      "epoch": 0.36004056795131845,
      "grad_norm": 1.3195017576217651,
      "learning_rate": 0.00017606490872210955,
      "loss": 2.8118,
      "step": 2130
    },
    {
      "epoch": 0.36173089925625423,
      "grad_norm": 2.66767954826355,
      "learning_rate": 0.00017595221996844716,
      "loss": 2.6391,
      "step": 2140
    },
    {
      "epoch": 0.36342123056118997,
      "grad_norm": 1.9061890840530396,
      "learning_rate": 0.0001758395312147848,
      "loss": 2.7792,
      "step": 2150
    },
    {
      "epoch": 0.36511156186612576,
      "grad_norm": 1.5182253122329712,
      "learning_rate": 0.0001757268424611224,
      "loss": 2.7316,
      "step": 2160
    },
    {
      "epoch": 0.36680189317106154,
      "grad_norm": 1.4128228425979614,
      "learning_rate": 0.00017561415370746,
      "loss": 2.5812,
      "step": 2170
    },
    {
      "epoch": 0.3684922244759973,
      "grad_norm": 1.7591472864151,
      "learning_rate": 0.00017550146495379763,
      "loss": 2.8188,
      "step": 2180
    },
    {
      "epoch": 0.37018255578093306,
      "grad_norm": 1.656294822692871,
      "learning_rate": 0.00017538877620013524,
      "loss": 2.6099,
      "step": 2190
    },
    {
      "epoch": 0.37187288708586885,
      "grad_norm": 2.8715498447418213,
      "learning_rate": 0.00017527608744647287,
      "loss": 2.7007,
      "step": 2200
    },
    {
      "epoch": 0.3735632183908046,
      "grad_norm": 1.7107751369476318,
      "learning_rate": 0.00017516339869281047,
      "loss": 2.7606,
      "step": 2210
    },
    {
      "epoch": 0.3752535496957404,
      "grad_norm": 1.9605177640914917,
      "learning_rate": 0.00017505070993914808,
      "loss": 2.8763,
      "step": 2220
    },
    {
      "epoch": 0.3769438810006761,
      "grad_norm": 2.015523910522461,
      "learning_rate": 0.0001749380211854857,
      "loss": 2.8401,
      "step": 2230
    },
    {
      "epoch": 0.3786342123056119,
      "grad_norm": 1.4740149974822998,
      "learning_rate": 0.00017482533243182331,
      "loss": 2.9037,
      "step": 2240
    },
    {
      "epoch": 0.3803245436105477,
      "grad_norm": 1.1675982475280762,
      "learning_rate": 0.00017471264367816095,
      "loss": 2.672,
      "step": 2250
    },
    {
      "epoch": 0.3820148749154834,
      "grad_norm": 1.653464913368225,
      "learning_rate": 0.00017459995492449855,
      "loss": 2.6429,
      "step": 2260
    },
    {
      "epoch": 0.3837052062204192,
      "grad_norm": 1.4600919485092163,
      "learning_rate": 0.00017448726617083616,
      "loss": 2.744,
      "step": 2270
    },
    {
      "epoch": 0.385395537525355,
      "grad_norm": 1.4130175113677979,
      "learning_rate": 0.0001743745774171738,
      "loss": 2.6346,
      "step": 2280
    },
    {
      "epoch": 0.3870858688302907,
      "grad_norm": 1.8675180673599243,
      "learning_rate": 0.0001742618886635114,
      "loss": 2.6587,
      "step": 2290
    },
    {
      "epoch": 0.3887762001352265,
      "grad_norm": 1.8563517332077026,
      "learning_rate": 0.00017414919990984903,
      "loss": 2.6776,
      "step": 2300
    },
    {
      "epoch": 0.39046653144016225,
      "grad_norm": 1.4356874227523804,
      "learning_rate": 0.00017403651115618663,
      "loss": 2.7773,
      "step": 2310
    },
    {
      "epoch": 0.39215686274509803,
      "grad_norm": 1.3816019296646118,
      "learning_rate": 0.00017392382240252424,
      "loss": 2.8693,
      "step": 2320
    },
    {
      "epoch": 0.3938471940500338,
      "grad_norm": 2.5362462997436523,
      "learning_rate": 0.00017381113364886187,
      "loss": 2.7945,
      "step": 2330
    },
    {
      "epoch": 0.39553752535496955,
      "grad_norm": 1.2649644613265991,
      "learning_rate": 0.00017369844489519947,
      "loss": 2.7421,
      "step": 2340
    },
    {
      "epoch": 0.39722785665990534,
      "grad_norm": 1.9831386804580688,
      "learning_rate": 0.0001735857561415371,
      "loss": 2.6606,
      "step": 2350
    },
    {
      "epoch": 0.39891818796484113,
      "grad_norm": 1.4844555854797363,
      "learning_rate": 0.0001734730673878747,
      "loss": 2.6866,
      "step": 2360
    },
    {
      "epoch": 0.40060851926977686,
      "grad_norm": 2.516779899597168,
      "learning_rate": 0.00017336037863421232,
      "loss": 2.703,
      "step": 2370
    },
    {
      "epoch": 0.40229885057471265,
      "grad_norm": 1.306825876235962,
      "learning_rate": 0.00017324768988054995,
      "loss": 2.7871,
      "step": 2380
    },
    {
      "epoch": 0.4039891818796484,
      "grad_norm": 2.809860944747925,
      "learning_rate": 0.00017313500112688755,
      "loss": 2.6885,
      "step": 2390
    },
    {
      "epoch": 0.4056795131845842,
      "grad_norm": 1.184940218925476,
      "learning_rate": 0.00017302231237322516,
      "loss": 2.8488,
      "step": 2400
    },
    {
      "epoch": 0.40736984448951996,
      "grad_norm": 1.4394004344940186,
      "learning_rate": 0.0001729096236195628,
      "loss": 2.6496,
      "step": 2410
    },
    {
      "epoch": 0.4090601757944557,
      "grad_norm": 2.081617832183838,
      "learning_rate": 0.0001727969348659004,
      "loss": 2.6308,
      "step": 2420
    },
    {
      "epoch": 0.4107505070993915,
      "grad_norm": 1.6256073713302612,
      "learning_rate": 0.000172684246112238,
      "loss": 2.5875,
      "step": 2430
    },
    {
      "epoch": 0.41244083840432727,
      "grad_norm": 1.4481428861618042,
      "learning_rate": 0.00017257155735857563,
      "loss": 2.6665,
      "step": 2440
    },
    {
      "epoch": 0.414131169709263,
      "grad_norm": 1.987339735031128,
      "learning_rate": 0.00017245886860491324,
      "loss": 2.8377,
      "step": 2450
    },
    {
      "epoch": 0.4158215010141988,
      "grad_norm": 1.1505036354064941,
      "learning_rate": 0.00017234617985125084,
      "loss": 2.8459,
      "step": 2460
    },
    {
      "epoch": 0.4175118323191345,
      "grad_norm": 1.4828393459320068,
      "learning_rate": 0.00017223349109758847,
      "loss": 2.6791,
      "step": 2470
    },
    {
      "epoch": 0.4192021636240703,
      "grad_norm": 1.4524743556976318,
      "learning_rate": 0.00017212080234392608,
      "loss": 2.7388,
      "step": 2480
    },
    {
      "epoch": 0.4208924949290061,
      "grad_norm": 2.4045281410217285,
      "learning_rate": 0.00017200811359026368,
      "loss": 2.678,
      "step": 2490
    },
    {
      "epoch": 0.42258282623394183,
      "grad_norm": 1.3499526977539062,
      "learning_rate": 0.00017189542483660132,
      "loss": 2.8018,
      "step": 2500
    },
    {
      "epoch": 0.4242731575388776,
      "grad_norm": 2.2734551429748535,
      "learning_rate": 0.00017178273608293892,
      "loss": 2.8476,
      "step": 2510
    },
    {
      "epoch": 0.4259634888438134,
      "grad_norm": 1.1570987701416016,
      "learning_rate": 0.00017167004732927655,
      "loss": 2.6781,
      "step": 2520
    },
    {
      "epoch": 0.42765382014874914,
      "grad_norm": 1.8363534212112427,
      "learning_rate": 0.00017155735857561416,
      "loss": 2.6912,
      "step": 2530
    },
    {
      "epoch": 0.42934415145368493,
      "grad_norm": 1.479819893836975,
      "learning_rate": 0.00017144466982195176,
      "loss": 2.7476,
      "step": 2540
    },
    {
      "epoch": 0.43103448275862066,
      "grad_norm": 1.365755558013916,
      "learning_rate": 0.0001713319810682894,
      "loss": 2.7786,
      "step": 2550
    },
    {
      "epoch": 0.43272481406355645,
      "grad_norm": 2.229004144668579,
      "learning_rate": 0.000171219292314627,
      "loss": 2.6652,
      "step": 2560
    },
    {
      "epoch": 0.43441514536849224,
      "grad_norm": 1.7759883403778076,
      "learning_rate": 0.0001711066035609646,
      "loss": 2.7037,
      "step": 2570
    },
    {
      "epoch": 0.43610547667342797,
      "grad_norm": 0.9641414284706116,
      "learning_rate": 0.00017099391480730224,
      "loss": 2.7086,
      "step": 2580
    },
    {
      "epoch": 0.43779580797836376,
      "grad_norm": 1.8764811754226685,
      "learning_rate": 0.00017088122605363984,
      "loss": 2.7855,
      "step": 2590
    },
    {
      "epoch": 0.43948613928329955,
      "grad_norm": 1.4361006021499634,
      "learning_rate": 0.00017076853729997745,
      "loss": 2.7478,
      "step": 2600
    },
    {
      "epoch": 0.4411764705882353,
      "grad_norm": 2.1596877574920654,
      "learning_rate": 0.00017065584854631508,
      "loss": 2.6682,
      "step": 2610
    },
    {
      "epoch": 0.44286680189317107,
      "grad_norm": 2.3703432083129883,
      "learning_rate": 0.00017054315979265268,
      "loss": 2.7067,
      "step": 2620
    },
    {
      "epoch": 0.4445571331981068,
      "grad_norm": 2.3089346885681152,
      "learning_rate": 0.00017043047103899032,
      "loss": 2.9094,
      "step": 2630
    },
    {
      "epoch": 0.4462474645030426,
      "grad_norm": 1.6969008445739746,
      "learning_rate": 0.00017031778228532792,
      "loss": 2.7023,
      "step": 2640
    },
    {
      "epoch": 0.4479377958079784,
      "grad_norm": 1.7626665830612183,
      "learning_rate": 0.00017020509353166553,
      "loss": 2.7258,
      "step": 2650
    },
    {
      "epoch": 0.4496281271129141,
      "grad_norm": 1.432391881942749,
      "learning_rate": 0.00017009240477800316,
      "loss": 2.6752,
      "step": 2660
    },
    {
      "epoch": 0.4513184584178499,
      "grad_norm": 1.1275157928466797,
      "learning_rate": 0.00016997971602434076,
      "loss": 2.871,
      "step": 2670
    },
    {
      "epoch": 0.4530087897227857,
      "grad_norm": 1.6529228687286377,
      "learning_rate": 0.0001698670272706784,
      "loss": 2.7674,
      "step": 2680
    },
    {
      "epoch": 0.4546991210277214,
      "grad_norm": 1.3886255025863647,
      "learning_rate": 0.000169754338517016,
      "loss": 2.5409,
      "step": 2690
    },
    {
      "epoch": 0.4563894523326572,
      "grad_norm": 1.105648159980774,
      "learning_rate": 0.0001696416497633536,
      "loss": 2.6458,
      "step": 2700
    },
    {
      "epoch": 0.45807978363759294,
      "grad_norm": 1.3307385444641113,
      "learning_rate": 0.00016952896100969124,
      "loss": 2.8307,
      "step": 2710
    },
    {
      "epoch": 0.45977011494252873,
      "grad_norm": 1.8082919120788574,
      "learning_rate": 0.00016941627225602884,
      "loss": 2.6469,
      "step": 2720
    },
    {
      "epoch": 0.4614604462474645,
      "grad_norm": 1.5452407598495483,
      "learning_rate": 0.00016930358350236648,
      "loss": 2.8592,
      "step": 2730
    },
    {
      "epoch": 0.46315077755240025,
      "grad_norm": 1.974893569946289,
      "learning_rate": 0.00016919089474870408,
      "loss": 2.7412,
      "step": 2740
    },
    {
      "epoch": 0.46484110885733604,
      "grad_norm": 1.5173351764678955,
      "learning_rate": 0.00016907820599504169,
      "loss": 2.61,
      "step": 2750
    },
    {
      "epoch": 0.4665314401622718,
      "grad_norm": 1.2629300355911255,
      "learning_rate": 0.00016896551724137932,
      "loss": 2.553,
      "step": 2760
    },
    {
      "epoch": 0.46822177146720756,
      "grad_norm": 1.6081842184066772,
      "learning_rate": 0.00016885282848771692,
      "loss": 2.886,
      "step": 2770
    },
    {
      "epoch": 0.46991210277214335,
      "grad_norm": 1.5914934873580933,
      "learning_rate": 0.00016874013973405456,
      "loss": 2.8435,
      "step": 2780
    },
    {
      "epoch": 0.4716024340770791,
      "grad_norm": 1.2829070091247559,
      "learning_rate": 0.00016862745098039216,
      "loss": 2.7266,
      "step": 2790
    },
    {
      "epoch": 0.47329276538201487,
      "grad_norm": 1.6539515256881714,
      "learning_rate": 0.00016851476222672977,
      "loss": 2.7339,
      "step": 2800
    },
    {
      "epoch": 0.47498309668695066,
      "grad_norm": 1.82451331615448,
      "learning_rate": 0.0001684020734730674,
      "loss": 2.7779,
      "step": 2810
    },
    {
      "epoch": 0.4766734279918864,
      "grad_norm": 1.8496136665344238,
      "learning_rate": 0.000168289384719405,
      "loss": 2.8241,
      "step": 2820
    },
    {
      "epoch": 0.4783637592968222,
      "grad_norm": 1.2951922416687012,
      "learning_rate": 0.00016817669596574263,
      "loss": 2.6607,
      "step": 2830
    },
    {
      "epoch": 0.48005409060175797,
      "grad_norm": 1.572056770324707,
      "learning_rate": 0.00016806400721208024,
      "loss": 2.7702,
      "step": 2840
    },
    {
      "epoch": 0.4817444219066937,
      "grad_norm": 1.881478190422058,
      "learning_rate": 0.00016795131845841784,
      "loss": 2.7518,
      "step": 2850
    },
    {
      "epoch": 0.4834347532116295,
      "grad_norm": 1.7010976076126099,
      "learning_rate": 0.00016783862970475548,
      "loss": 2.674,
      "step": 2860
    },
    {
      "epoch": 0.4851250845165652,
      "grad_norm": 1.6052783727645874,
      "learning_rate": 0.00016772594095109308,
      "loss": 2.6587,
      "step": 2870
    },
    {
      "epoch": 0.486815415821501,
      "grad_norm": 1.7871615886688232,
      "learning_rate": 0.00016761325219743071,
      "loss": 2.6458,
      "step": 2880
    },
    {
      "epoch": 0.4885057471264368,
      "grad_norm": 1.2913333177566528,
      "learning_rate": 0.00016750056344376832,
      "loss": 2.7183,
      "step": 2890
    },
    {
      "epoch": 0.49019607843137253,
      "grad_norm": 1.401696801185608,
      "learning_rate": 0.00016738787469010592,
      "loss": 2.766,
      "step": 2900
    },
    {
      "epoch": 0.4918864097363083,
      "grad_norm": 1.3599985837936401,
      "learning_rate": 0.00016727518593644356,
      "loss": 2.618,
      "step": 2910
    },
    {
      "epoch": 0.4935767410412441,
      "grad_norm": 1.540022373199463,
      "learning_rate": 0.00016716249718278116,
      "loss": 2.7586,
      "step": 2920
    },
    {
      "epoch": 0.49526707234617984,
      "grad_norm": 1.5780272483825684,
      "learning_rate": 0.0001670498084291188,
      "loss": 2.8079,
      "step": 2930
    },
    {
      "epoch": 0.4969574036511156,
      "grad_norm": 1.3297730684280396,
      "learning_rate": 0.0001669371196754564,
      "loss": 2.7796,
      "step": 2940
    },
    {
      "epoch": 0.49864773495605136,
      "grad_norm": 0.9751949906349182,
      "learning_rate": 0.000166824430921794,
      "loss": 2.7134,
      "step": 2950
    },
    {
      "epoch": 0.5003380662609872,
      "grad_norm": 1.9848507642745972,
      "learning_rate": 0.00016671174216813164,
      "loss": 2.6492,
      "step": 2960
    },
    {
      "epoch": 0.5020283975659229,
      "grad_norm": 1.4382705688476562,
      "learning_rate": 0.00016659905341446924,
      "loss": 2.7128,
      "step": 2970
    },
    {
      "epoch": 0.5037187288708587,
      "grad_norm": 1.6320066452026367,
      "learning_rate": 0.00016648636466080685,
      "loss": 2.8517,
      "step": 2980
    },
    {
      "epoch": 0.5054090601757945,
      "grad_norm": 1.8193674087524414,
      "learning_rate": 0.00016637367590714448,
      "loss": 2.6787,
      "step": 2990
    },
    {
      "epoch": 0.5070993914807302,
      "grad_norm": 1.7573493719100952,
      "learning_rate": 0.00016626098715348208,
      "loss": 2.7108,
      "step": 3000
    },
    {
      "epoch": 0.508789722785666,
      "grad_norm": 1.4926728010177612,
      "learning_rate": 0.00016614829839981971,
      "loss": 2.7574,
      "step": 3010
    },
    {
      "epoch": 0.5104800540906017,
      "grad_norm": 1.0925228595733643,
      "learning_rate": 0.00016603560964615732,
      "loss": 2.5589,
      "step": 3020
    },
    {
      "epoch": 0.5121703853955375,
      "grad_norm": 1.6632251739501953,
      "learning_rate": 0.00016592292089249492,
      "loss": 2.7972,
      "step": 3030
    },
    {
      "epoch": 0.5138607167004733,
      "grad_norm": 1.1922951936721802,
      "learning_rate": 0.00016581023213883256,
      "loss": 2.8701,
      "step": 3040
    },
    {
      "epoch": 0.5155510480054091,
      "grad_norm": 1.7588253021240234,
      "learning_rate": 0.00016569754338517016,
      "loss": 2.5491,
      "step": 3050
    },
    {
      "epoch": 0.5172413793103449,
      "grad_norm": 1.8092201948165894,
      "learning_rate": 0.0001655848546315078,
      "loss": 2.7563,
      "step": 3060
    },
    {
      "epoch": 0.5189317106152807,
      "grad_norm": 1.873666524887085,
      "learning_rate": 0.0001654721658778454,
      "loss": 2.7281,
      "step": 3070
    },
    {
      "epoch": 0.5206220419202163,
      "grad_norm": 1.0588362216949463,
      "learning_rate": 0.000165359477124183,
      "loss": 2.6092,
      "step": 3080
    },
    {
      "epoch": 0.5223123732251521,
      "grad_norm": 1.1658750772476196,
      "learning_rate": 0.00016524678837052064,
      "loss": 2.7634,
      "step": 3090
    },
    {
      "epoch": 0.5240027045300879,
      "grad_norm": 1.104887843132019,
      "learning_rate": 0.00016513409961685824,
      "loss": 2.7118,
      "step": 3100
    },
    {
      "epoch": 0.5256930358350237,
      "grad_norm": 1.0639137029647827,
      "learning_rate": 0.00016502141086319587,
      "loss": 2.8043,
      "step": 3110
    },
    {
      "epoch": 0.5273833671399595,
      "grad_norm": 1.9175430536270142,
      "learning_rate": 0.00016490872210953348,
      "loss": 2.9159,
      "step": 3120
    },
    {
      "epoch": 0.5290736984448952,
      "grad_norm": 1.599757432937622,
      "learning_rate": 0.00016479603335587108,
      "loss": 2.682,
      "step": 3130
    },
    {
      "epoch": 0.530764029749831,
      "grad_norm": 1.4782520532608032,
      "learning_rate": 0.00016468334460220872,
      "loss": 2.7068,
      "step": 3140
    },
    {
      "epoch": 0.5324543610547667,
      "grad_norm": 1.8675308227539062,
      "learning_rate": 0.00016457065584854632,
      "loss": 2.8039,
      "step": 3150
    },
    {
      "epoch": 0.5341446923597025,
      "grad_norm": 1.4821727275848389,
      "learning_rate": 0.00016445796709488395,
      "loss": 2.701,
      "step": 3160
    },
    {
      "epoch": 0.5358350236646383,
      "grad_norm": 1.777905821800232,
      "learning_rate": 0.00016434527834122156,
      "loss": 2.7319,
      "step": 3170
    },
    {
      "epoch": 0.537525354969574,
      "grad_norm": 1.782997727394104,
      "learning_rate": 0.00016423258958755916,
      "loss": 2.7887,
      "step": 3180
    },
    {
      "epoch": 0.5392156862745098,
      "grad_norm": 1.1057158708572388,
      "learning_rate": 0.0001641199008338968,
      "loss": 2.7178,
      "step": 3190
    },
    {
      "epoch": 0.5409060175794456,
      "grad_norm": 1.5601623058319092,
      "learning_rate": 0.0001640072120802344,
      "loss": 2.6987,
      "step": 3200
    },
    {
      "epoch": 0.5425963488843814,
      "grad_norm": 1.3482818603515625,
      "learning_rate": 0.00016389452332657203,
      "loss": 2.6801,
      "step": 3210
    },
    {
      "epoch": 0.5442866801893171,
      "grad_norm": 1.2023106813430786,
      "learning_rate": 0.00016378183457290964,
      "loss": 2.703,
      "step": 3220
    },
    {
      "epoch": 0.5459770114942529,
      "grad_norm": 1.4241504669189453,
      "learning_rate": 0.00016366914581924724,
      "loss": 2.7775,
      "step": 3230
    },
    {
      "epoch": 0.5476673427991886,
      "grad_norm": 1.8430614471435547,
      "learning_rate": 0.00016355645706558487,
      "loss": 2.5269,
      "step": 3240
    },
    {
      "epoch": 0.5493576741041244,
      "grad_norm": 1.5548171997070312,
      "learning_rate": 0.00016344376831192248,
      "loss": 2.6926,
      "step": 3250
    },
    {
      "epoch": 0.5510480054090602,
      "grad_norm": 1.3967645168304443,
      "learning_rate": 0.0001633310795582601,
      "loss": 2.6124,
      "step": 3260
    },
    {
      "epoch": 0.552738336713996,
      "grad_norm": 1.4478952884674072,
      "learning_rate": 0.00016321839080459772,
      "loss": 2.7562,
      "step": 3270
    },
    {
      "epoch": 0.5544286680189318,
      "grad_norm": 1.796532392501831,
      "learning_rate": 0.00016310570205093532,
      "loss": 2.7642,
      "step": 3280
    },
    {
      "epoch": 0.5561189993238674,
      "grad_norm": 1.149261236190796,
      "learning_rate": 0.00016299301329727295,
      "loss": 2.6825,
      "step": 3290
    },
    {
      "epoch": 0.5578093306288032,
      "grad_norm": 1.704258918762207,
      "learning_rate": 0.00016288032454361056,
      "loss": 2.6262,
      "step": 3300
    },
    {
      "epoch": 0.559499661933739,
      "grad_norm": 1.7026195526123047,
      "learning_rate": 0.0001627676357899482,
      "loss": 2.6921,
      "step": 3310
    },
    {
      "epoch": 0.5611899932386748,
      "grad_norm": 1.2876489162445068,
      "learning_rate": 0.0001626549470362858,
      "loss": 2.6808,
      "step": 3320
    },
    {
      "epoch": 0.5628803245436106,
      "grad_norm": 1.4948145151138306,
      "learning_rate": 0.0001625422582826234,
      "loss": 2.7073,
      "step": 3330
    },
    {
      "epoch": 0.5645706558485463,
      "grad_norm": 1.536234736442566,
      "learning_rate": 0.00016242956952896103,
      "loss": 2.6371,
      "step": 3340
    },
    {
      "epoch": 0.566260987153482,
      "grad_norm": 1.6549897193908691,
      "learning_rate": 0.00016231688077529864,
      "loss": 2.7745,
      "step": 3350
    },
    {
      "epoch": 0.5679513184584178,
      "grad_norm": 1.0837942361831665,
      "learning_rate": 0.00016220419202163627,
      "loss": 2.6141,
      "step": 3360
    },
    {
      "epoch": 0.5696416497633536,
      "grad_norm": 1.9298266172409058,
      "learning_rate": 0.00016209150326797388,
      "loss": 2.5402,
      "step": 3370
    },
    {
      "epoch": 0.5713319810682894,
      "grad_norm": 1.761025071144104,
      "learning_rate": 0.00016197881451431148,
      "loss": 2.572,
      "step": 3380
    },
    {
      "epoch": 0.5730223123732252,
      "grad_norm": 1.7282400131225586,
      "learning_rate": 0.0001618661257606491,
      "loss": 2.8079,
      "step": 3390
    },
    {
      "epoch": 0.5747126436781609,
      "grad_norm": 1.2307109832763672,
      "learning_rate": 0.00016175343700698672,
      "loss": 2.747,
      "step": 3400
    },
    {
      "epoch": 0.5764029749830967,
      "grad_norm": 1.1986799240112305,
      "learning_rate": 0.00016164074825332435,
      "loss": 2.8211,
      "step": 3410
    },
    {
      "epoch": 0.5780933062880325,
      "grad_norm": 1.571686863899231,
      "learning_rate": 0.00016152805949966195,
      "loss": 2.5951,
      "step": 3420
    },
    {
      "epoch": 0.5797836375929682,
      "grad_norm": 1.733960509300232,
      "learning_rate": 0.00016141537074599956,
      "loss": 2.6521,
      "step": 3430
    },
    {
      "epoch": 0.581473968897904,
      "grad_norm": 2.216151237487793,
      "learning_rate": 0.0001613026819923372,
      "loss": 2.7295,
      "step": 3440
    },
    {
      "epoch": 0.5831643002028397,
      "grad_norm": 1.5344419479370117,
      "learning_rate": 0.0001611899932386748,
      "loss": 2.7186,
      "step": 3450
    },
    {
      "epoch": 0.5848546315077755,
      "grad_norm": 1.0784838199615479,
      "learning_rate": 0.0001610773044850124,
      "loss": 2.7202,
      "step": 3460
    },
    {
      "epoch": 0.5865449628127113,
      "grad_norm": 1.3459045886993408,
      "learning_rate": 0.00016096461573135003,
      "loss": 2.792,
      "step": 3470
    },
    {
      "epoch": 0.5882352941176471,
      "grad_norm": 2.032613754272461,
      "learning_rate": 0.00016085192697768764,
      "loss": 2.605,
      "step": 3480
    },
    {
      "epoch": 0.5899256254225829,
      "grad_norm": 1.8357033729553223,
      "learning_rate": 0.00016073923822402524,
      "loss": 2.5836,
      "step": 3490
    },
    {
      "epoch": 0.5916159567275185,
      "grad_norm": 1.4714577198028564,
      "learning_rate": 0.00016062654947036288,
      "loss": 2.5774,
      "step": 3500
    },
    {
      "epoch": 0.5933062880324543,
      "grad_norm": 1.3471324443817139,
      "learning_rate": 0.00016051386071670048,
      "loss": 2.7432,
      "step": 3510
    },
    {
      "epoch": 0.5949966193373901,
      "grad_norm": 1.6803468465805054,
      "learning_rate": 0.00016040117196303809,
      "loss": 2.7104,
      "step": 3520
    },
    {
      "epoch": 0.5966869506423259,
      "grad_norm": 1.7918027639389038,
      "learning_rate": 0.00016028848320937572,
      "loss": 2.7756,
      "step": 3530
    },
    {
      "epoch": 0.5983772819472617,
      "grad_norm": 2.1320505142211914,
      "learning_rate": 0.00016017579445571332,
      "loss": 2.6407,
      "step": 3540
    },
    {
      "epoch": 0.6000676132521975,
      "grad_norm": 1.6313364505767822,
      "learning_rate": 0.00016006310570205093,
      "loss": 2.5429,
      "step": 3550
    },
    {
      "epoch": 0.6017579445571332,
      "grad_norm": 1.8260862827301025,
      "learning_rate": 0.00015995041694838856,
      "loss": 2.8391,
      "step": 3560
    },
    {
      "epoch": 0.603448275862069,
      "grad_norm": 1.6663488149642944,
      "learning_rate": 0.00015983772819472617,
      "loss": 2.7344,
      "step": 3570
    },
    {
      "epoch": 0.6051386071670047,
      "grad_norm": 1.773459792137146,
      "learning_rate": 0.0001597250394410638,
      "loss": 2.7349,
      "step": 3580
    },
    {
      "epoch": 0.6068289384719405,
      "grad_norm": 1.4155230522155762,
      "learning_rate": 0.0001596123506874014,
      "loss": 2.6928,
      "step": 3590
    },
    {
      "epoch": 0.6085192697768763,
      "grad_norm": 1.3997976779937744,
      "learning_rate": 0.000159499661933739,
      "loss": 2.6311,
      "step": 3600
    },
    {
      "epoch": 0.610209601081812,
      "grad_norm": 0.936610221862793,
      "learning_rate": 0.00015938697318007664,
      "loss": 2.7892,
      "step": 3610
    },
    {
      "epoch": 0.6118999323867478,
      "grad_norm": 1.4694459438323975,
      "learning_rate": 0.00015927428442641424,
      "loss": 2.8103,
      "step": 3620
    },
    {
      "epoch": 0.6135902636916836,
      "grad_norm": 1.3146979808807373,
      "learning_rate": 0.00015916159567275185,
      "loss": 2.7204,
      "step": 3630
    },
    {
      "epoch": 0.6152805949966194,
      "grad_norm": 1.3312793970108032,
      "learning_rate": 0.00015904890691908948,
      "loss": 2.734,
      "step": 3640
    },
    {
      "epoch": 0.6169709263015551,
      "grad_norm": 1.5728784799575806,
      "learning_rate": 0.0001589362181654271,
      "loss": 2.6516,
      "step": 3650
    },
    {
      "epoch": 0.6186612576064908,
      "grad_norm": 2.0173182487487793,
      "learning_rate": 0.0001588235294117647,
      "loss": 2.7045,
      "step": 3660
    },
    {
      "epoch": 0.6203515889114266,
      "grad_norm": 1.9711225032806396,
      "learning_rate": 0.00015871084065810232,
      "loss": 2.681,
      "step": 3670
    },
    {
      "epoch": 0.6220419202163624,
      "grad_norm": 1.2432491779327393,
      "learning_rate": 0.00015859815190443993,
      "loss": 2.7332,
      "step": 3680
    },
    {
      "epoch": 0.6237322515212982,
      "grad_norm": 2.084630250930786,
      "learning_rate": 0.00015848546315077756,
      "loss": 2.6671,
      "step": 3690
    },
    {
      "epoch": 0.625422582826234,
      "grad_norm": 1.4293212890625,
      "learning_rate": 0.00015837277439711517,
      "loss": 2.7671,
      "step": 3700
    },
    {
      "epoch": 0.6271129141311698,
      "grad_norm": 1.1345901489257812,
      "learning_rate": 0.00015826008564345277,
      "loss": 2.7315,
      "step": 3710
    },
    {
      "epoch": 0.6288032454361054,
      "grad_norm": 2.1810731887817383,
      "learning_rate": 0.0001581473968897904,
      "loss": 2.7074,
      "step": 3720
    },
    {
      "epoch": 0.6304935767410412,
      "grad_norm": 1.3282781839370728,
      "learning_rate": 0.000158034708136128,
      "loss": 2.6781,
      "step": 3730
    },
    {
      "epoch": 0.632183908045977,
      "grad_norm": 2.088400363922119,
      "learning_rate": 0.00015792201938246564,
      "loss": 2.7628,
      "step": 3740
    },
    {
      "epoch": 0.6338742393509128,
      "grad_norm": 2.306452751159668,
      "learning_rate": 0.00015780933062880325,
      "loss": 2.6037,
      "step": 3750
    },
    {
      "epoch": 0.6355645706558486,
      "grad_norm": 1.1955152750015259,
      "learning_rate": 0.00015769664187514085,
      "loss": 2.6702,
      "step": 3760
    },
    {
      "epoch": 0.6372549019607843,
      "grad_norm": 1.3505982160568237,
      "learning_rate": 0.00015758395312147848,
      "loss": 2.7626,
      "step": 3770
    },
    {
      "epoch": 0.6389452332657201,
      "grad_norm": 1.8851498365402222,
      "learning_rate": 0.0001574712643678161,
      "loss": 2.5942,
      "step": 3780
    },
    {
      "epoch": 0.6406355645706558,
      "grad_norm": 1.2617546319961548,
      "learning_rate": 0.00015735857561415372,
      "loss": 2.6145,
      "step": 3790
    },
    {
      "epoch": 0.6423258958755916,
      "grad_norm": 1.278648018836975,
      "learning_rate": 0.00015724588686049132,
      "loss": 2.7651,
      "step": 3800
    },
    {
      "epoch": 0.6440162271805274,
      "grad_norm": 1.5138862133026123,
      "learning_rate": 0.00015713319810682893,
      "loss": 2.5476,
      "step": 3810
    },
    {
      "epoch": 0.6457065584854631,
      "grad_norm": 1.361928105354309,
      "learning_rate": 0.00015702050935316656,
      "loss": 2.6754,
      "step": 3820
    },
    {
      "epoch": 0.6473968897903989,
      "grad_norm": 1.6504508256912231,
      "learning_rate": 0.00015690782059950417,
      "loss": 2.6208,
      "step": 3830
    },
    {
      "epoch": 0.6490872210953347,
      "grad_norm": 1.821394681930542,
      "learning_rate": 0.0001567951318458418,
      "loss": 2.7512,
      "step": 3840
    },
    {
      "epoch": 0.6507775524002705,
      "grad_norm": 1.5101468563079834,
      "learning_rate": 0.0001566824430921794,
      "loss": 2.5987,
      "step": 3850
    },
    {
      "epoch": 0.6524678837052063,
      "grad_norm": 1.7227890491485596,
      "learning_rate": 0.000156569754338517,
      "loss": 2.7516,
      "step": 3860
    },
    {
      "epoch": 0.654158215010142,
      "grad_norm": 1.8931723833084106,
      "learning_rate": 0.00015645706558485464,
      "loss": 2.7229,
      "step": 3870
    },
    {
      "epoch": 0.6558485463150777,
      "grad_norm": 2.094698905944824,
      "learning_rate": 0.00015634437683119225,
      "loss": 2.6797,
      "step": 3880
    },
    {
      "epoch": 0.6575388776200135,
      "grad_norm": 1.3540748357772827,
      "learning_rate": 0.00015623168807752988,
      "loss": 2.7877,
      "step": 3890
    },
    {
      "epoch": 0.6592292089249493,
      "grad_norm": 1.2919528484344482,
      "learning_rate": 0.00015611899932386748,
      "loss": 2.5348,
      "step": 3900
    },
    {
      "epoch": 0.6609195402298851,
      "grad_norm": 1.8746623992919922,
      "learning_rate": 0.0001560063105702051,
      "loss": 2.762,
      "step": 3910
    },
    {
      "epoch": 0.6626098715348209,
      "grad_norm": 1.5281606912612915,
      "learning_rate": 0.00015589362181654272,
      "loss": 2.6422,
      "step": 3920
    },
    {
      "epoch": 0.6643002028397565,
      "grad_norm": 1.6810765266418457,
      "learning_rate": 0.00015578093306288033,
      "loss": 2.7451,
      "step": 3930
    },
    {
      "epoch": 0.6659905341446923,
      "grad_norm": 1.311633825302124,
      "learning_rate": 0.00015566824430921793,
      "loss": 2.766,
      "step": 3940
    },
    {
      "epoch": 0.6676808654496281,
      "grad_norm": 1.2650067806243896,
      "learning_rate": 0.00015555555555555556,
      "loss": 2.686,
      "step": 3950
    },
    {
      "epoch": 0.6693711967545639,
      "grad_norm": 1.5424631834030151,
      "learning_rate": 0.00015544286680189317,
      "loss": 2.86,
      "step": 3960
    },
    {
      "epoch": 0.6710615280594997,
      "grad_norm": 1.707004189491272,
      "learning_rate": 0.0001553301780482308,
      "loss": 2.6484,
      "step": 3970
    },
    {
      "epoch": 0.6727518593644354,
      "grad_norm": 1.4508823156356812,
      "learning_rate": 0.0001552174892945684,
      "loss": 2.775,
      "step": 3980
    },
    {
      "epoch": 0.6744421906693712,
      "grad_norm": 1.2727835178375244,
      "learning_rate": 0.000155104800540906,
      "loss": 2.8217,
      "step": 3990
    },
    {
      "epoch": 0.676132521974307,
      "grad_norm": 1.5777575969696045,
      "learning_rate": 0.00015499211178724364,
      "loss": 2.667,
      "step": 4000
    },
    {
      "epoch": 0.6778228532792427,
      "grad_norm": 1.283841609954834,
      "learning_rate": 0.00015487942303358125,
      "loss": 2.6561,
      "step": 4010
    },
    {
      "epoch": 0.6795131845841785,
      "grad_norm": 1.4381664991378784,
      "learning_rate": 0.00015476673427991888,
      "loss": 2.7677,
      "step": 4020
    },
    {
      "epoch": 0.6812035158891143,
      "grad_norm": 1.1337084770202637,
      "learning_rate": 0.00015465404552625648,
      "loss": 2.5695,
      "step": 4030
    },
    {
      "epoch": 0.68289384719405,
      "grad_norm": 1.5295525789260864,
      "learning_rate": 0.0001545413567725941,
      "loss": 2.7333,
      "step": 4040
    },
    {
      "epoch": 0.6845841784989858,
      "grad_norm": 1.7359936237335205,
      "learning_rate": 0.00015442866801893172,
      "loss": 2.6234,
      "step": 4050
    },
    {
      "epoch": 0.6862745098039216,
      "grad_norm": 1.527740716934204,
      "learning_rate": 0.00015431597926526933,
      "loss": 2.5836,
      "step": 4060
    },
    {
      "epoch": 0.6879648411088574,
      "grad_norm": 1.5360052585601807,
      "learning_rate": 0.00015420329051160696,
      "loss": 2.8344,
      "step": 4070
    },
    {
      "epoch": 0.6896551724137931,
      "grad_norm": 2.4926068782806396,
      "learning_rate": 0.00015409060175794456,
      "loss": 2.7121,
      "step": 4080
    },
    {
      "epoch": 0.6913455037187288,
      "grad_norm": 2.0683352947235107,
      "learning_rate": 0.00015397791300428217,
      "loss": 2.6792,
      "step": 4090
    },
    {
      "epoch": 0.6930358350236646,
      "grad_norm": 1.4059638977050781,
      "learning_rate": 0.0001538652242506198,
      "loss": 2.7899,
      "step": 4100
    },
    {
      "epoch": 0.6947261663286004,
      "grad_norm": 1.6329946517944336,
      "learning_rate": 0.0001537525354969574,
      "loss": 2.615,
      "step": 4110
    },
    {
      "epoch": 0.6964164976335362,
      "grad_norm": 1.8991377353668213,
      "learning_rate": 0.00015363984674329504,
      "loss": 2.7332,
      "step": 4120
    },
    {
      "epoch": 0.698106828938472,
      "grad_norm": 1.5773108005523682,
      "learning_rate": 0.00015352715798963264,
      "loss": 2.717,
      "step": 4130
    },
    {
      "epoch": 0.6997971602434077,
      "grad_norm": 1.7017911672592163,
      "learning_rate": 0.00015341446923597025,
      "loss": 2.7412,
      "step": 4140
    },
    {
      "epoch": 0.7014874915483434,
      "grad_norm": 1.9174848794937134,
      "learning_rate": 0.00015330178048230788,
      "loss": 2.7466,
      "step": 4150
    },
    {
      "epoch": 0.7031778228532792,
      "grad_norm": 1.601893663406372,
      "learning_rate": 0.00015318909172864549,
      "loss": 2.6775,
      "step": 4160
    },
    {
      "epoch": 0.704868154158215,
      "grad_norm": 1.5304992198944092,
      "learning_rate": 0.00015307640297498312,
      "loss": 2.6452,
      "step": 4170
    },
    {
      "epoch": 0.7065584854631508,
      "grad_norm": 1.3918486833572388,
      "learning_rate": 0.00015296371422132072,
      "loss": 2.563,
      "step": 4180
    },
    {
      "epoch": 0.7082488167680866,
      "grad_norm": 1.8047454357147217,
      "learning_rate": 0.00015285102546765833,
      "loss": 2.5576,
      "step": 4190
    },
    {
      "epoch": 0.7099391480730223,
      "grad_norm": 1.2481566667556763,
      "learning_rate": 0.00015273833671399596,
      "loss": 2.5742,
      "step": 4200
    },
    {
      "epoch": 0.7116294793779581,
      "grad_norm": 1.5758824348449707,
      "learning_rate": 0.00015262564796033356,
      "loss": 2.7383,
      "step": 4210
    },
    {
      "epoch": 0.7133198106828939,
      "grad_norm": 1.685976266860962,
      "learning_rate": 0.0001525129592066712,
      "loss": 2.5857,
      "step": 4220
    },
    {
      "epoch": 0.7150101419878296,
      "grad_norm": 1.5055512189865112,
      "learning_rate": 0.0001524002704530088,
      "loss": 2.63,
      "step": 4230
    },
    {
      "epoch": 0.7167004732927654,
      "grad_norm": 1.8368752002716064,
      "learning_rate": 0.0001522875816993464,
      "loss": 2.6818,
      "step": 4240
    },
    {
      "epoch": 0.7183908045977011,
      "grad_norm": 1.2364779710769653,
      "learning_rate": 0.00015217489294568404,
      "loss": 2.7509,
      "step": 4250
    },
    {
      "epoch": 0.7200811359026369,
      "grad_norm": 1.2037725448608398,
      "learning_rate": 0.00015206220419202164,
      "loss": 2.7504,
      "step": 4260
    },
    {
      "epoch": 0.7217714672075727,
      "grad_norm": 1.683671236038208,
      "learning_rate": 0.00015194951543835928,
      "loss": 2.7253,
      "step": 4270
    },
    {
      "epoch": 0.7234617985125085,
      "grad_norm": 1.9890873432159424,
      "learning_rate": 0.00015183682668469688,
      "loss": 2.8194,
      "step": 4280
    },
    {
      "epoch": 0.7251521298174443,
      "grad_norm": 1.6378434896469116,
      "learning_rate": 0.00015172413793103449,
      "loss": 2.7267,
      "step": 4290
    },
    {
      "epoch": 0.7268424611223799,
      "grad_norm": 1.7498232126235962,
      "learning_rate": 0.00015161144917737212,
      "loss": 2.6448,
      "step": 4300
    },
    {
      "epoch": 0.7285327924273157,
      "grad_norm": 1.4529682397842407,
      "learning_rate": 0.00015149876042370972,
      "loss": 2.6851,
      "step": 4310
    },
    {
      "epoch": 0.7302231237322515,
      "grad_norm": 1.282194972038269,
      "learning_rate": 0.00015138607167004736,
      "loss": 2.6848,
      "step": 4320
    },
    {
      "epoch": 0.7319134550371873,
      "grad_norm": 1.209381341934204,
      "learning_rate": 0.00015127338291638496,
      "loss": 2.5723,
      "step": 4330
    },
    {
      "epoch": 0.7336037863421231,
      "grad_norm": 1.469732403755188,
      "learning_rate": 0.00015116069416272257,
      "loss": 2.6284,
      "step": 4340
    },
    {
      "epoch": 0.7352941176470589,
      "grad_norm": 1.562057375907898,
      "learning_rate": 0.0001510480054090602,
      "loss": 2.5884,
      "step": 4350
    },
    {
      "epoch": 0.7369844489519946,
      "grad_norm": 1.332198143005371,
      "learning_rate": 0.0001509353166553978,
      "loss": 2.6485,
      "step": 4360
    },
    {
      "epoch": 0.7386747802569303,
      "grad_norm": 1.7286250591278076,
      "learning_rate": 0.00015082262790173543,
      "loss": 2.6901,
      "step": 4370
    },
    {
      "epoch": 0.7403651115618661,
      "grad_norm": 1.3828085660934448,
      "learning_rate": 0.00015070993914807304,
      "loss": 2.7551,
      "step": 4380
    },
    {
      "epoch": 0.7420554428668019,
      "grad_norm": 1.4886804819107056,
      "learning_rate": 0.00015059725039441064,
      "loss": 2.6675,
      "step": 4390
    },
    {
      "epoch": 0.7437457741717377,
      "grad_norm": 1.8076142072677612,
      "learning_rate": 0.00015048456164074828,
      "loss": 2.7446,
      "step": 4400
    },
    {
      "epoch": 0.7454361054766734,
      "grad_norm": 1.7777771949768066,
      "learning_rate": 0.00015037187288708588,
      "loss": 2.641,
      "step": 4410
    },
    {
      "epoch": 0.7471264367816092,
      "grad_norm": 1.5617915391921997,
      "learning_rate": 0.0001502591841334235,
      "loss": 2.5643,
      "step": 4420
    },
    {
      "epoch": 0.748816768086545,
      "grad_norm": 2.0688912868499756,
      "learning_rate": 0.00015014649537976112,
      "loss": 2.6736,
      "step": 4430
    },
    {
      "epoch": 0.7505070993914807,
      "grad_norm": 1.5447447299957275,
      "learning_rate": 0.00015003380662609872,
      "loss": 2.6988,
      "step": 4440
    },
    {
      "epoch": 0.7521974306964165,
      "grad_norm": 1.3481959104537964,
      "learning_rate": 0.00014992111787243636,
      "loss": 2.6475,
      "step": 4450
    },
    {
      "epoch": 0.7538877620013522,
      "grad_norm": 1.8889857530593872,
      "learning_rate": 0.00014980842911877396,
      "loss": 2.6076,
      "step": 4460
    },
    {
      "epoch": 0.755578093306288,
      "grad_norm": 1.4563753604888916,
      "learning_rate": 0.00014969574036511157,
      "loss": 2.6771,
      "step": 4470
    },
    {
      "epoch": 0.7572684246112238,
      "grad_norm": 2.4414584636688232,
      "learning_rate": 0.0001495830516114492,
      "loss": 2.6708,
      "step": 4480
    },
    {
      "epoch": 0.7589587559161596,
      "grad_norm": 1.4976005554199219,
      "learning_rate": 0.0001494703628577868,
      "loss": 2.6564,
      "step": 4490
    },
    {
      "epoch": 0.7606490872210954,
      "grad_norm": 1.6085940599441528,
      "learning_rate": 0.00014935767410412444,
      "loss": 2.5005,
      "step": 4500
    },
    {
      "epoch": 0.7623394185260312,
      "grad_norm": 2.076306104660034,
      "learning_rate": 0.00014924498535046204,
      "loss": 2.7833,
      "step": 4510
    },
    {
      "epoch": 0.7640297498309668,
      "grad_norm": 1.6233747005462646,
      "learning_rate": 0.00014913229659679965,
      "loss": 2.6377,
      "step": 4520
    },
    {
      "epoch": 0.7657200811359026,
      "grad_norm": 1.9524486064910889,
      "learning_rate": 0.00014901960784313728,
      "loss": 2.7941,
      "step": 4530
    },
    {
      "epoch": 0.7674104124408384,
      "grad_norm": 1.1133618354797363,
      "learning_rate": 0.00014890691908947488,
      "loss": 2.7425,
      "step": 4540
    },
    {
      "epoch": 0.7691007437457742,
      "grad_norm": 1.2927402257919312,
      "learning_rate": 0.0001487942303358125,
      "loss": 2.6751,
      "step": 4550
    },
    {
      "epoch": 0.77079107505071,
      "grad_norm": 1.823144793510437,
      "learning_rate": 0.00014868154158215012,
      "loss": 2.5308,
      "step": 4560
    },
    {
      "epoch": 0.7724814063556457,
      "grad_norm": 1.306452751159668,
      "learning_rate": 0.00014856885282848773,
      "loss": 2.7307,
      "step": 4570
    },
    {
      "epoch": 0.7741717376605814,
      "grad_norm": 1.8654274940490723,
      "learning_rate": 0.00014845616407482533,
      "loss": 2.6945,
      "step": 4580
    },
    {
      "epoch": 0.7758620689655172,
      "grad_norm": 1.0372506380081177,
      "learning_rate": 0.00014834347532116296,
      "loss": 2.7497,
      "step": 4590
    },
    {
      "epoch": 0.777552400270453,
      "grad_norm": 2.219372272491455,
      "learning_rate": 0.00014823078656750057,
      "loss": 2.7382,
      "step": 4600
    },
    {
      "epoch": 0.7792427315753888,
      "grad_norm": 1.565522313117981,
      "learning_rate": 0.00014811809781383817,
      "loss": 2.6979,
      "step": 4610
    },
    {
      "epoch": 0.7809330628803245,
      "grad_norm": 1.2367136478424072,
      "learning_rate": 0.0001480054090601758,
      "loss": 2.3923,
      "step": 4620
    },
    {
      "epoch": 0.7826233941852603,
      "grad_norm": 1.683751106262207,
      "learning_rate": 0.0001478927203065134,
      "loss": 2.759,
      "step": 4630
    },
    {
      "epoch": 0.7843137254901961,
      "grad_norm": 1.3612667322158813,
      "learning_rate": 0.00014778003155285104,
      "loss": 2.7148,
      "step": 4640
    },
    {
      "epoch": 0.7860040567951319,
      "grad_norm": 1.6782252788543701,
      "learning_rate": 0.00014766734279918865,
      "loss": 2.6478,
      "step": 4650
    },
    {
      "epoch": 0.7876943881000676,
      "grad_norm": 1.5772058963775635,
      "learning_rate": 0.00014755465404552625,
      "loss": 2.7639,
      "step": 4660
    },
    {
      "epoch": 0.7893847194050034,
      "grad_norm": 1.5905712842941284,
      "learning_rate": 0.00014744196529186388,
      "loss": 2.6123,
      "step": 4670
    },
    {
      "epoch": 0.7910750507099391,
      "grad_norm": 1.258631944656372,
      "learning_rate": 0.0001473292765382015,
      "loss": 2.8405,
      "step": 4680
    },
    {
      "epoch": 0.7927653820148749,
      "grad_norm": 1.001252293586731,
      "learning_rate": 0.0001472165877845391,
      "loss": 2.7297,
      "step": 4690
    },
    {
      "epoch": 0.7944557133198107,
      "grad_norm": 1.5797927379608154,
      "learning_rate": 0.00014710389903087673,
      "loss": 2.552,
      "step": 4700
    },
    {
      "epoch": 0.7961460446247465,
      "grad_norm": 1.9237881898880005,
      "learning_rate": 0.00014699121027721433,
      "loss": 2.6383,
      "step": 4710
    },
    {
      "epoch": 0.7978363759296823,
      "grad_norm": 2.0226924419403076,
      "learning_rate": 0.00014687852152355194,
      "loss": 2.6646,
      "step": 4720
    },
    {
      "epoch": 0.7995267072346179,
      "grad_norm": 1.187285304069519,
      "learning_rate": 0.00014676583276988957,
      "loss": 2.8646,
      "step": 4730
    },
    {
      "epoch": 0.8012170385395537,
      "grad_norm": 2.1187407970428467,
      "learning_rate": 0.00014665314401622717,
      "loss": 2.7131,
      "step": 4740
    },
    {
      "epoch": 0.8029073698444895,
      "grad_norm": 1.2420059442520142,
      "learning_rate": 0.0001465404552625648,
      "loss": 2.7758,
      "step": 4750
    },
    {
      "epoch": 0.8045977011494253,
      "grad_norm": 1.5772874355316162,
      "learning_rate": 0.0001464277665089024,
      "loss": 2.6604,
      "step": 4760
    },
    {
      "epoch": 0.8062880324543611,
      "grad_norm": 1.294828176498413,
      "learning_rate": 0.00014631507775524002,
      "loss": 2.6643,
      "step": 4770
    },
    {
      "epoch": 0.8079783637592968,
      "grad_norm": 1.8192322254180908,
      "learning_rate": 0.00014620238900157765,
      "loss": 2.7219,
      "step": 4780
    },
    {
      "epoch": 0.8096686950642326,
      "grad_norm": 1.1737476587295532,
      "learning_rate": 0.00014608970024791525,
      "loss": 2.6172,
      "step": 4790
    },
    {
      "epoch": 0.8113590263691683,
      "grad_norm": 2.040081739425659,
      "learning_rate": 0.00014597701149425288,
      "loss": 2.7048,
      "step": 4800
    },
    {
      "epoch": 0.8130493576741041,
      "grad_norm": 1.135993480682373,
      "learning_rate": 0.0001458643227405905,
      "loss": 2.7577,
      "step": 4810
    },
    {
      "epoch": 0.8147396889790399,
      "grad_norm": 1.445837140083313,
      "learning_rate": 0.0001457516339869281,
      "loss": 2.6159,
      "step": 4820
    },
    {
      "epoch": 0.8164300202839757,
      "grad_norm": 1.2759828567504883,
      "learning_rate": 0.00014563894523326573,
      "loss": 2.6246,
      "step": 4830
    },
    {
      "epoch": 0.8181203515889114,
      "grad_norm": 1.6241215467453003,
      "learning_rate": 0.00014552625647960333,
      "loss": 2.7738,
      "step": 4840
    },
    {
      "epoch": 0.8198106828938472,
      "grad_norm": 1.5214306116104126,
      "learning_rate": 0.00014541356772594096,
      "loss": 2.6539,
      "step": 4850
    },
    {
      "epoch": 0.821501014198783,
      "grad_norm": 2.0814414024353027,
      "learning_rate": 0.00014530087897227857,
      "loss": 2.6707,
      "step": 4860
    },
    {
      "epoch": 0.8231913455037188,
      "grad_norm": 2.2871317863464355,
      "learning_rate": 0.00014518819021861617,
      "loss": 2.6362,
      "step": 4870
    },
    {
      "epoch": 0.8248816768086545,
      "grad_norm": 1.9230197668075562,
      "learning_rate": 0.0001450755014649538,
      "loss": 2.6096,
      "step": 4880
    },
    {
      "epoch": 0.8265720081135902,
      "grad_norm": 1.917378544807434,
      "learning_rate": 0.0001449628127112914,
      "loss": 2.6898,
      "step": 4890
    },
    {
      "epoch": 0.828262339418526,
      "grad_norm": 1.9673359394073486,
      "learning_rate": 0.00014485012395762902,
      "loss": 2.6903,
      "step": 4900
    },
    {
      "epoch": 0.8299526707234618,
      "grad_norm": 1.2879700660705566,
      "learning_rate": 0.00014473743520396665,
      "loss": 2.6974,
      "step": 4910
    },
    {
      "epoch": 0.8316430020283976,
      "grad_norm": 1.246856451034546,
      "learning_rate": 0.00014462474645030425,
      "loss": 2.5368,
      "step": 4920
    },
    {
      "epoch": 0.8333333333333334,
      "grad_norm": 1.3082653284072876,
      "learning_rate": 0.00014451205769664189,
      "loss": 2.6824,
      "step": 4930
    },
    {
      "epoch": 0.835023664638269,
      "grad_norm": 2.5569941997528076,
      "learning_rate": 0.0001443993689429795,
      "loss": 2.6186,
      "step": 4940
    },
    {
      "epoch": 0.8367139959432048,
      "grad_norm": 1.2385077476501465,
      "learning_rate": 0.0001442866801893171,
      "loss": 2.6553,
      "step": 4950
    },
    {
      "epoch": 0.8384043272481406,
      "grad_norm": 1.4538401365280151,
      "learning_rate": 0.00014417399143565473,
      "loss": 2.7699,
      "step": 4960
    },
    {
      "epoch": 0.8400946585530764,
      "grad_norm": 1.8715914487838745,
      "learning_rate": 0.00014406130268199233,
      "loss": 2.6241,
      "step": 4970
    },
    {
      "epoch": 0.8417849898580122,
      "grad_norm": 1.4182237386703491,
      "learning_rate": 0.00014394861392832996,
      "loss": 2.6096,
      "step": 4980
    },
    {
      "epoch": 0.843475321162948,
      "grad_norm": 2.0724198818206787,
      "learning_rate": 0.00014383592517466757,
      "loss": 2.7198,
      "step": 4990
    },
    {
      "epoch": 0.8451656524678837,
      "grad_norm": 2.0016977787017822,
      "learning_rate": 0.00014372323642100518,
      "loss": 2.505,
      "step": 5000
    },
    {
      "epoch": 0.8468559837728195,
      "grad_norm": 1.902042269706726,
      "learning_rate": 0.0001436105476673428,
      "loss": 2.614,
      "step": 5010
    },
    {
      "epoch": 0.8485463150777552,
      "grad_norm": 1.466499924659729,
      "learning_rate": 0.0001434978589136804,
      "loss": 2.7958,
      "step": 5020
    },
    {
      "epoch": 0.850236646382691,
      "grad_norm": 1.1220035552978516,
      "learning_rate": 0.00014338517016001804,
      "loss": 2.6614,
      "step": 5030
    },
    {
      "epoch": 0.8519269776876268,
      "grad_norm": 1.7194398641586304,
      "learning_rate": 0.00014327248140635565,
      "loss": 2.733,
      "step": 5040
    },
    {
      "epoch": 0.8536173089925625,
      "grad_norm": 1.239848256111145,
      "learning_rate": 0.00014315979265269325,
      "loss": 2.726,
      "step": 5050
    },
    {
      "epoch": 0.8553076402974983,
      "grad_norm": 1.6838151216506958,
      "learning_rate": 0.0001430471038990309,
      "loss": 2.571,
      "step": 5060
    },
    {
      "epoch": 0.8569979716024341,
      "grad_norm": 1.2990480661392212,
      "learning_rate": 0.0001429344151453685,
      "loss": 2.534,
      "step": 5070
    },
    {
      "epoch": 0.8586883029073699,
      "grad_norm": 1.4778666496276855,
      "learning_rate": 0.00014282172639170612,
      "loss": 2.6894,
      "step": 5080
    },
    {
      "epoch": 0.8603786342123056,
      "grad_norm": 1.239522099494934,
      "learning_rate": 0.00014270903763804373,
      "loss": 2.6681,
      "step": 5090
    },
    {
      "epoch": 0.8620689655172413,
      "grad_norm": 1.4735498428344727,
      "learning_rate": 0.00014259634888438133,
      "loss": 2.7412,
      "step": 5100
    },
    {
      "epoch": 0.8637592968221771,
      "grad_norm": 1.291136384010315,
      "learning_rate": 0.00014248366013071897,
      "loss": 2.6923,
      "step": 5110
    },
    {
      "epoch": 0.8654496281271129,
      "grad_norm": 1.2122007608413696,
      "learning_rate": 0.00014237097137705657,
      "loss": 2.5928,
      "step": 5120
    },
    {
      "epoch": 0.8671399594320487,
      "grad_norm": 1.3328266143798828,
      "learning_rate": 0.0001422582826233942,
      "loss": 2.6262,
      "step": 5130
    },
    {
      "epoch": 0.8688302907369845,
      "grad_norm": 1.5500043630599976,
      "learning_rate": 0.0001421455938697318,
      "loss": 2.819,
      "step": 5140
    },
    {
      "epoch": 0.8705206220419203,
      "grad_norm": 1.575551152229309,
      "learning_rate": 0.0001420329051160694,
      "loss": 2.7538,
      "step": 5150
    },
    {
      "epoch": 0.8722109533468559,
      "grad_norm": 1.19022798538208,
      "learning_rate": 0.00014192021636240705,
      "loss": 2.6639,
      "step": 5160
    },
    {
      "epoch": 0.8739012846517917,
      "grad_norm": 0.9997943043708801,
      "learning_rate": 0.00014180752760874465,
      "loss": 2.7799,
      "step": 5170
    },
    {
      "epoch": 0.8755916159567275,
      "grad_norm": 1.608236312866211,
      "learning_rate": 0.00014169483885508228,
      "loss": 2.6691,
      "step": 5180
    },
    {
      "epoch": 0.8772819472616633,
      "grad_norm": 1.0941696166992188,
      "learning_rate": 0.0001415821501014199,
      "loss": 2.6254,
      "step": 5190
    },
    {
      "epoch": 0.8789722785665991,
      "grad_norm": 1.2832752466201782,
      "learning_rate": 0.0001414694613477575,
      "loss": 2.5171,
      "step": 5200
    },
    {
      "epoch": 0.8806626098715348,
      "grad_norm": 1.3918551206588745,
      "learning_rate": 0.00014135677259409512,
      "loss": 2.6786,
      "step": 5210
    },
    {
      "epoch": 0.8823529411764706,
      "grad_norm": 1.3743007183074951,
      "learning_rate": 0.00014124408384043273,
      "loss": 2.6127,
      "step": 5220
    },
    {
      "epoch": 0.8840432724814064,
      "grad_norm": 1.5081197023391724,
      "learning_rate": 0.00014113139508677036,
      "loss": 2.6027,
      "step": 5230
    },
    {
      "epoch": 0.8857336037863421,
      "grad_norm": 1.6335113048553467,
      "learning_rate": 0.00014101870633310797,
      "loss": 2.6336,
      "step": 5240
    },
    {
      "epoch": 0.8874239350912779,
      "grad_norm": 1.868441104888916,
      "learning_rate": 0.00014090601757944557,
      "loss": 2.671,
      "step": 5250
    },
    {
      "epoch": 0.8891142663962136,
      "grad_norm": 1.5355554819107056,
      "learning_rate": 0.0001407933288257832,
      "loss": 2.6612,
      "step": 5260
    },
    {
      "epoch": 0.8908045977011494,
      "grad_norm": 1.3841402530670166,
      "learning_rate": 0.0001406806400721208,
      "loss": 2.6709,
      "step": 5270
    },
    {
      "epoch": 0.8924949290060852,
      "grad_norm": 1.7290503978729248,
      "learning_rate": 0.00014056795131845844,
      "loss": 2.6891,
      "step": 5280
    },
    {
      "epoch": 0.894185260311021,
      "grad_norm": 1.4509367942810059,
      "learning_rate": 0.00014045526256479605,
      "loss": 2.6049,
      "step": 5290
    },
    {
      "epoch": 0.8958755916159568,
      "grad_norm": 2.331296920776367,
      "learning_rate": 0.00014034257381113365,
      "loss": 2.7976,
      "step": 5300
    },
    {
      "epoch": 0.8975659229208925,
      "grad_norm": 1.753061056137085,
      "learning_rate": 0.00014022988505747128,
      "loss": 2.6535,
      "step": 5310
    },
    {
      "epoch": 0.8992562542258282,
      "grad_norm": 1.4926888942718506,
      "learning_rate": 0.0001401171963038089,
      "loss": 2.6421,
      "step": 5320
    },
    {
      "epoch": 0.900946585530764,
      "grad_norm": 1.330133318901062,
      "learning_rate": 0.00014000450755014652,
      "loss": 2.6361,
      "step": 5330
    },
    {
      "epoch": 0.9026369168356998,
      "grad_norm": 1.3442137241363525,
      "learning_rate": 0.00013989181879648413,
      "loss": 2.9272,
      "step": 5340
    },
    {
      "epoch": 0.9043272481406356,
      "grad_norm": 1.4213401079177856,
      "learning_rate": 0.00013977913004282173,
      "loss": 2.7293,
      "step": 5350
    },
    {
      "epoch": 0.9060175794455714,
      "grad_norm": 1.503956913948059,
      "learning_rate": 0.00013966644128915936,
      "loss": 2.6669,
      "step": 5360
    },
    {
      "epoch": 0.907707910750507,
      "grad_norm": 2.128864049911499,
      "learning_rate": 0.00013955375253549697,
      "loss": 2.5099,
      "step": 5370
    },
    {
      "epoch": 0.9093982420554428,
      "grad_norm": 1.4971294403076172,
      "learning_rate": 0.00013944106378183457,
      "loss": 2.6792,
      "step": 5380
    },
    {
      "epoch": 0.9110885733603786,
      "grad_norm": 1.185659408569336,
      "learning_rate": 0.0001393283750281722,
      "loss": 2.5966,
      "step": 5390
    },
    {
      "epoch": 0.9127789046653144,
      "grad_norm": 1.6782667636871338,
      "learning_rate": 0.0001392156862745098,
      "loss": 2.6915,
      "step": 5400
    },
    {
      "epoch": 0.9144692359702502,
      "grad_norm": 1.6775342226028442,
      "learning_rate": 0.00013910299752084744,
      "loss": 2.6876,
      "step": 5410
    },
    {
      "epoch": 0.9161595672751859,
      "grad_norm": 1.105081558227539,
      "learning_rate": 0.00013899030876718505,
      "loss": 2.6636,
      "step": 5420
    },
    {
      "epoch": 0.9178498985801217,
      "grad_norm": 1.6747872829437256,
      "learning_rate": 0.00013887762001352265,
      "loss": 2.9206,
      "step": 5430
    },
    {
      "epoch": 0.9195402298850575,
      "grad_norm": 1.0126395225524902,
      "learning_rate": 0.00013876493125986028,
      "loss": 2.6314,
      "step": 5440
    },
    {
      "epoch": 0.9212305611899932,
      "grad_norm": 1.130448341369629,
      "learning_rate": 0.0001386522425061979,
      "loss": 2.685,
      "step": 5450
    },
    {
      "epoch": 0.922920892494929,
      "grad_norm": 1.325831413269043,
      "learning_rate": 0.00013853955375253552,
      "loss": 2.6628,
      "step": 5460
    },
    {
      "epoch": 0.9246112237998648,
      "grad_norm": 1.4533720016479492,
      "learning_rate": 0.00013842686499887313,
      "loss": 2.7752,
      "step": 5470
    },
    {
      "epoch": 0.9263015551048005,
      "grad_norm": 1.4814845323562622,
      "learning_rate": 0.00013831417624521073,
      "loss": 2.6768,
      "step": 5480
    },
    {
      "epoch": 0.9279918864097363,
      "grad_norm": 2.047919988632202,
      "learning_rate": 0.00013820148749154836,
      "loss": 2.8824,
      "step": 5490
    },
    {
      "epoch": 0.9296822177146721,
      "grad_norm": 1.425315260887146,
      "learning_rate": 0.00013808879873788597,
      "loss": 2.7158,
      "step": 5500
    },
    {
      "epoch": 0.9313725490196079,
      "grad_norm": 1.6709086894989014,
      "learning_rate": 0.0001379761099842236,
      "loss": 2.6784,
      "step": 5510
    },
    {
      "epoch": 0.9330628803245437,
      "grad_norm": 1.7651914358139038,
      "learning_rate": 0.0001378634212305612,
      "loss": 2.6576,
      "step": 5520
    },
    {
      "epoch": 0.9347532116294793,
      "grad_norm": 1.3017168045043945,
      "learning_rate": 0.0001377507324768988,
      "loss": 2.7569,
      "step": 5530
    },
    {
      "epoch": 0.9364435429344151,
      "grad_norm": 1.2257843017578125,
      "learning_rate": 0.00013763804372323644,
      "loss": 2.7957,
      "step": 5540
    },
    {
      "epoch": 0.9381338742393509,
      "grad_norm": 1.6178624629974365,
      "learning_rate": 0.00013752535496957405,
      "loss": 2.6894,
      "step": 5550
    },
    {
      "epoch": 0.9398242055442867,
      "grad_norm": 1.352555513381958,
      "learning_rate": 0.00013741266621591168,
      "loss": 2.7709,
      "step": 5560
    },
    {
      "epoch": 0.9415145368492225,
      "grad_norm": 1.3227490186691284,
      "learning_rate": 0.00013729997746224928,
      "loss": 2.7956,
      "step": 5570
    },
    {
      "epoch": 0.9432048681541582,
      "grad_norm": 1.478069543838501,
      "learning_rate": 0.0001371872887085869,
      "loss": 2.6334,
      "step": 5580
    },
    {
      "epoch": 0.944895199459094,
      "grad_norm": 1.1021236181259155,
      "learning_rate": 0.00013707459995492452,
      "loss": 2.7543,
      "step": 5590
    },
    {
      "epoch": 0.9465855307640297,
      "grad_norm": 1.7451064586639404,
      "learning_rate": 0.00013696191120126213,
      "loss": 2.6027,
      "step": 5600
    },
    {
      "epoch": 0.9482758620689655,
      "grad_norm": 1.6655011177062988,
      "learning_rate": 0.00013684922244759973,
      "loss": 2.5506,
      "step": 5610
    },
    {
      "epoch": 0.9499661933739013,
      "grad_norm": 2.010514736175537,
      "learning_rate": 0.00013673653369393736,
      "loss": 2.6686,
      "step": 5620
    },
    {
      "epoch": 0.9516565246788371,
      "grad_norm": 1.350826382637024,
      "learning_rate": 0.00013662384494027497,
      "loss": 2.7103,
      "step": 5630
    },
    {
      "epoch": 0.9533468559837728,
      "grad_norm": 1.8904614448547363,
      "learning_rate": 0.00013651115618661257,
      "loss": 2.5854,
      "step": 5640
    },
    {
      "epoch": 0.9550371872887086,
      "grad_norm": 2.3255116939544678,
      "learning_rate": 0.0001363984674329502,
      "loss": 2.6384,
      "step": 5650
    },
    {
      "epoch": 0.9567275185936444,
      "grad_norm": 1.4741896390914917,
      "learning_rate": 0.0001362857786792878,
      "loss": 2.7976,
      "step": 5660
    },
    {
      "epoch": 0.9584178498985801,
      "grad_norm": 1.5385228395462036,
      "learning_rate": 0.00013617308992562542,
      "loss": 2.8446,
      "step": 5670
    },
    {
      "epoch": 0.9601081812035159,
      "grad_norm": 2.09427809715271,
      "learning_rate": 0.00013606040117196305,
      "loss": 2.5762,
      "step": 5680
    },
    {
      "epoch": 0.9617985125084516,
      "grad_norm": 1.9228147268295288,
      "learning_rate": 0.00013594771241830065,
      "loss": 2.4981,
      "step": 5690
    },
    {
      "epoch": 0.9634888438133874,
      "grad_norm": 1.9102734327316284,
      "learning_rate": 0.00013583502366463829,
      "loss": 2.7606,
      "step": 5700
    },
    {
      "epoch": 0.9651791751183232,
      "grad_norm": 1.0931243896484375,
      "learning_rate": 0.0001357223349109759,
      "loss": 2.5055,
      "step": 5710
    },
    {
      "epoch": 0.966869506423259,
      "grad_norm": 1.6007473468780518,
      "learning_rate": 0.0001356096461573135,
      "loss": 2.6542,
      "step": 5720
    },
    {
      "epoch": 0.9685598377281948,
      "grad_norm": 2.0499463081359863,
      "learning_rate": 0.00013549695740365113,
      "loss": 2.5899,
      "step": 5730
    },
    {
      "epoch": 0.9702501690331304,
      "grad_norm": 2.0426297187805176,
      "learning_rate": 0.00013538426864998873,
      "loss": 2.5942,
      "step": 5740
    },
    {
      "epoch": 0.9719405003380662,
      "grad_norm": 1.8986907005310059,
      "learning_rate": 0.00013527157989632634,
      "loss": 2.8383,
      "step": 5750
    },
    {
      "epoch": 0.973630831643002,
      "grad_norm": 2.0977625846862793,
      "learning_rate": 0.00013515889114266397,
      "loss": 2.7352,
      "step": 5760
    },
    {
      "epoch": 0.9753211629479378,
      "grad_norm": 1.2816334962844849,
      "learning_rate": 0.00013504620238900158,
      "loss": 2.7215,
      "step": 5770
    },
    {
      "epoch": 0.9770114942528736,
      "grad_norm": 1.5631005764007568,
      "learning_rate": 0.00013493351363533918,
      "loss": 2.6902,
      "step": 5780
    },
    {
      "epoch": 0.9787018255578094,
      "grad_norm": 1.2881876230239868,
      "learning_rate": 0.0001348208248816768,
      "loss": 2.5805,
      "step": 5790
    },
    {
      "epoch": 0.9803921568627451,
      "grad_norm": 1.236082673072815,
      "learning_rate": 0.00013470813612801442,
      "loss": 2.6337,
      "step": 5800
    },
    {
      "epoch": 0.9820824881676808,
      "grad_norm": 1.4468046426773071,
      "learning_rate": 0.00013459544737435205,
      "loss": 2.711,
      "step": 5810
    },
    {
      "epoch": 0.9837728194726166,
      "grad_norm": 2.2543606758117676,
      "learning_rate": 0.00013448275862068965,
      "loss": 2.8259,
      "step": 5820
    },
    {
      "epoch": 0.9854631507775524,
      "grad_norm": 1.5289136171340942,
      "learning_rate": 0.00013437006986702726,
      "loss": 2.5997,
      "step": 5830
    },
    {
      "epoch": 0.9871534820824882,
      "grad_norm": 1.1752219200134277,
      "learning_rate": 0.0001342573811133649,
      "loss": 2.7014,
      "step": 5840
    },
    {
      "epoch": 0.9888438133874239,
      "grad_norm": 1.1711899042129517,
      "learning_rate": 0.0001341446923597025,
      "loss": 2.7258,
      "step": 5850
    },
    {
      "epoch": 0.9905341446923597,
      "grad_norm": 1.0912375450134277,
      "learning_rate": 0.0001340320036060401,
      "loss": 2.6383,
      "step": 5860
    },
    {
      "epoch": 0.9922244759972955,
      "grad_norm": 1.3516439199447632,
      "learning_rate": 0.00013391931485237773,
      "loss": 2.6498,
      "step": 5870
    },
    {
      "epoch": 0.9939148073022313,
      "grad_norm": 1.3499284982681274,
      "learning_rate": 0.00013380662609871534,
      "loss": 2.6149,
      "step": 5880
    },
    {
      "epoch": 0.995605138607167,
      "grad_norm": 1.4246033430099487,
      "learning_rate": 0.00013369393734505297,
      "loss": 2.727,
      "step": 5890
    },
    {
      "epoch": 0.9972954699121027,
      "grad_norm": 1.655023217201233,
      "learning_rate": 0.00013358124859139058,
      "loss": 2.6898,
      "step": 5900
    },
    {
      "epoch": 0.9989858012170385,
      "grad_norm": 2.0825419425964355,
      "learning_rate": 0.00013346855983772818,
      "loss": 2.6084,
      "step": 5910
    },
    {
      "epoch": 1.0,
      "eval_loss": 2.7013356685638428,
      "eval_runtime": 4926.9862,
      "eval_samples_per_second": 1.201,
      "eval_steps_per_second": 0.3,
      "step": 5916
    }
  ],
  "logging_steps": 10,
  "max_steps": 17748,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 7.080258529394688e+16,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
